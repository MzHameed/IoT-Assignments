{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65922e73-ffed-4d4d-992e-b2efcf46f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model: QuantModel(\n",
      "  (quant): Quantize(scale=tensor([0.0039]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (conv): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.0026957429945468903, zero_point=0)\n",
      "  (bn): Identity()\n",
      "  (relu): Identity()\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "Quantized conv weight: tensor([[[[-0.8536]]]], size=(1, 1, 1, 1), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0067], dtype=torch.float64), zero_point=tensor([0]),\n",
      "       axis=0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub, get_default_qconfig\n",
    "\n",
    "# Set backend\n",
    "torch.backends.quantized.engine = 'onednn'\n",
    "\n",
    "# Model definition with layers that can be fused\n",
    "class QuantModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.conv = nn.Conv2d(1, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.dequant(x)\n",
    "\n",
    "# Prepare model\n",
    "model = QuantModel().eval()\n",
    "model.qconfig = get_default_qconfig('onednn')\n",
    "\n",
    "# Fuse Conv+BN+ReLU\n",
    "model = torch.ao.quantization.fuse_modules(\n",
    "    model,\n",
    "    [['conv', 'bn', 'relu']]  # Now this matches a known fusion pattern\n",
    ")\n",
    "\n",
    "# Prepare, calibrate, convert\n",
    "model_prepared = torch.ao.quantization.prepare(model)\n",
    "with torch.no_grad():\n",
    "    model_prepared(torch.rand(1, 1, 3, 3))  # Calibration\n",
    "    model_quantized = torch.ao.quantization.convert(model_prepared)  # Note indentation\n",
    "\n",
    "print(\"Quantized model:\", model_quantized)\n",
    "print(\"Quantized conv weight:\", model_quantized.conv.weight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18664755-726d-4481-adc2-a35d27436e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static quantization failed: quantized::qconv_prepack: ONEDNN only supports symmetric quantization of weight, whose zero point must be 0.\n",
      "Falling back to dynamic quantization for Linear layers only...\n",
      "Dynamic quantization applied to Linear layers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub, QConfig, MinMaxObserver\n",
    "\n",
    "# Force CPU for quantization\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 1. Load the model\n",
    "model_fp32 = models.resnet18(weights=\"IMAGENET1K_V1\").eval().to(device)\n",
    "\n",
    "# 2. Model wrapper for quantization\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super().__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "model = QuantizedResNet18(model_fp32).eval().to(device)\n",
    "\n",
    "# 3. Create custom QConfig since no backend is available\n",
    "custom_qconfig = QConfig(\n",
    "    activation=MinMaxObserver.with_args(dtype=torch.quint8),\n",
    "    weight=MinMaxObserver.with_args(dtype=torch.qint8)\n",
    ")\n",
    "\n",
    "model.qconfig = custom_qconfig\n",
    "\n",
    "# 4. Fuse modules (simplified example)\n",
    "model_fused = torch.ao.quantization.fuse_modules(model, [\n",
    "    ['model_fp32.conv1', 'model_fp32.bn1', 'model_fp32.relu'],\n",
    "    # Add other layers similarly\n",
    "])\n",
    "\n",
    "# 5. Prepare for quantization\n",
    "model_prepared = torch.ao.quantization.prepare(model_fused)\n",
    "\n",
    "# 6. Calibrate (using random data as example)\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        model_prepared(torch.randn(1, 3, 224, 224).to(device))\n",
    "\n",
    "# 7. Convert to quantized model\n",
    "try:\n",
    "    model_quantized = torch.ao.quantization.convert(model_prepared)\n",
    "    print(\"Static quantization successful!\")\n",
    "    \n",
    "    # Test inference\n",
    "    with torch.no_grad():\n",
    "        output = model_quantized(torch.randn(1, 3, 224, 224))\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        \n",
    "    # Save model\n",
    "    torch.save(model_quantized.state_dict(), \"quantized_resnet18_custom.pth\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    print(f\"Static quantization failed: {e}\")\n",
    "    print(\"Falling back to dynamic quantization for Linear layers only...\")\n",
    "    \n",
    "    # Dynamic quantization fallback\n",
    "    model_quantized = torch.quantization.quantize_dynamic(\n",
    "        model_fp32.to('cpu'), \n",
    "        {nn.Linear}, \n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    torch.save(model_quantized.state_dict(), \"quantized_resnet18_dynamic.pth\")\n",
    "    print(\"Dynamic quantization applied to Linear layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfc90a4-12b7-4db6-aaf9-054c7e741d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static quantization failed: quantized::qconv_prepack: ONEDNN only supports symmetric quantization of weight, whose zero point must be 0.\n",
      "Falling back to dynamic quantization...\n",
      "Dynamic quantization applied to Linear and Conv layers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub, QConfig, MinMaxObserver, fuse_modules\n",
    "\n",
    "# Force CPU for quantization\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 1. Load the model\n",
    "model_fp32 = models.resnet18(weights=\"IMAGENET1K_V1\").eval().to(device)\n",
    "\n",
    "# 2. Model wrapper for quantization\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super().__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "model = QuantizedResNet18(model_fp32).eval().to(device)\n",
    "\n",
    "# 3. Create custom QConfig\n",
    "custom_qconfig = QConfig(\n",
    "    activation=MinMaxObserver.with_args(\n",
    "        dtype=torch.quint8,\n",
    "        quant_min=0,\n",
    "        quant_max=255,\n",
    "        reduce_range=True  # Important for x86 CPUs\n",
    "    ),\n",
    "    weight=MinMaxObserver.with_args(\n",
    "        dtype=torch.qint8,\n",
    "        quant_min=-128,\n",
    "        quant_max=127\n",
    "    )\n",
    ")\n",
    "\n",
    "model.qconfig = custom_qconfig\n",
    "\n",
    "# 4. Fuse modules - must match exact patterns in ResNet18\n",
    "def fuse_resnet18(model):\n",
    "    # First fuse Conv+BN+ReLU\n",
    "    fuse_modules(model.model_fp32, \n",
    "                [['conv1', 'bn1', 'relu']], \n",
    "                inplace=True)\n",
    "    \n",
    "    # Then fuse all residual blocks\n",
    "    for name, module in model.model_fp32.named_children():\n",
    "        if name.startswith('layer'):\n",
    "            for basic_block in module:\n",
    "                fuse_modules(basic_block,\n",
    "                           [['conv1', 'bn1'], \n",
    "                            ['conv2', 'bn2']],\n",
    "                           inplace=True)\n",
    "    return model\n",
    "\n",
    "model_fused = fuse_resnet18(model)\n",
    "\n",
    "# 5. Prepare for quantization\n",
    "model_prepared = torch.ao.quantization.prepare(model_fused)\n",
    "\n",
    "# 6. Better calibration with more representative data\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # More calibration steps\n",
    "        # Use ImageNet-like input stats (normalized)\n",
    "        calib_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        calib_input = (calib_input * 0.5) + 0.5  # Simulate normalized input\n",
    "        model_prepared(calib_input)\n",
    "\n",
    "# 7. Convert to quantized model\n",
    "try:\n",
    "    model_quantized = torch.ao.quantization.convert(model_prepared)\n",
    "    print(\"Static quantization successful!\")\n",
    "    \n",
    "    # Test inference\n",
    "    with torch.no_grad():\n",
    "        test_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        output = model_quantized(test_input)\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        \n",
    "    # Save model (state_dict only - full model save not supported for quantized models)\n",
    "    torch.save(model_quantized.state_dict(), \"quantized_resnet18_custom.pth\")\n",
    "    print(\"Model saved successfully.\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    print(f\"Static quantization failed: {e}\")\n",
    "    print(\"Falling back to dynamic quantization...\")\n",
    "    \n",
    "    # Dynamic quantization fallback for Linear layers\n",
    "    model_quantized = torch.quantization.quantize_dynamic(\n",
    "        model_fp32.to('cpu'), \n",
    "        {nn.Linear, nn.Conv2d},  # Quantize both Linear and Conv layers\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    torch.save(model_quantized.state_dict(), \"quantized_resnet18_dynamic.pth\")\n",
    "    print(\"Dynamic quantization applied to Linear and Conv layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f71fb1-be6e-4e59-8efb-51757e46c6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIU856526097\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::conv2d_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Initialize quantized model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing quantized model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_static_quant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatic quantization complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Dataset class\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m, in \u001b[0;36mprepare_static_quant_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m         model_prepared(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Convert to quantized model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m model_quantized \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_fx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_quantized\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:620\u001b[0m, in \u001b[0;36mconvert_fx\u001b[1;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\" Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    617\u001b[0m \n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    619\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:547\u001b[0m, in \u001b[0;36m_convert_fx\u001b[1;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[0;32m    543\u001b[0m     convert_custom_config \u001b[38;5;241m=\u001b[39m ConvertCustomConfig\u001b[38;5;241m.\u001b[39mfrom_dict(convert_custom_config)\n\u001b[0;32m    545\u001b[0m _check_is_graph_module(graph_module)\n\u001b[1;32m--> 547\u001b[0m quantized \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m preserved_attributes \u001b[38;5;241m=\u001b[39m convert_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr_name \u001b[38;5;129;01min\u001b[39;00m preserved_attributes:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\convert.py:807\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# TODO: maybe move this to quantize_fx.py\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reference:\n\u001b[1;32m--> 807\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mlower_to_fbgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# remove this\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# removes qconfig and activation_post_process modules\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remove_qconfig_flag:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\lower_to_fbgemm.py:16\u001b[0m, in \u001b[0;36mlower_to_fbgemm\u001b[1;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower_to_fbgemm\u001b[39m(\n\u001b[0;32m      9\u001b[0m     model: QuantizedGraphModule,\n\u001b[0;32m     10\u001b[0m     qconfig_map: Dict[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m     11\u001b[0m     node_name_to_scope: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]]\n\u001b[0;32m     12\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuantizedGraphModule:\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    to fbgemm\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lower_to_native_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:928\u001b[0m, in \u001b[0;36m_lower_to_native_backend\u001b[1;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lower_to_native_backend\u001b[39m(\n\u001b[0;32m    920\u001b[0m     model: QuantizedGraphModule,\n\u001b[0;32m    921\u001b[0m     qconfig_map: Dict[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m    922\u001b[0m     node_name_to_scope: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]]\n\u001b[0;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuantizedGraphModule:\n\u001b[0;32m    924\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m    to the native backend in PyTorch (fbgemm/qnnpack), both backends shares the same\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m    operator signature so they can be lowered with the same function\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 928\u001b[0m     \u001b[43m_lower_static_weighted_ref_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    929\u001b[0m     _lower_dynamic_weighted_ref_module(model)\n\u001b[0;32m    930\u001b[0m     _lower_weight_only_weighted_ref_module(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:499\u001b[0m, in \u001b[0;36m_lower_static_weighted_ref_module\u001b[1;34m(model, qconfig_map)\u001b[0m\n\u001b[0;32m    497\u001b[0m output_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, scale_node\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    498\u001b[0m output_zero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, zero_point_node\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[1;32m--> 499\u001b[0m q_module \u001b[38;5;241m=\u001b[39m \u001b[43mq_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_zero_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# replace reference module with quantized module\u001b[39;00m\n\u001b[0;32m    501\u001b[0m parent_name, module_name \u001b[38;5;241m=\u001b[39m _parent_name(ref_node\u001b[38;5;241m.\u001b[39mtarget)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:110\u001b[0m, in \u001b[0;36mConvReLU2d.from_reference\u001b[1;34m(cls, ref_qconv, output_scale, output_zero_point)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_reference\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ref_qconv, output_scale, output_zero_point):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(ref_qconv) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mintrinsic\u001b[38;5;241m.\u001b[39mConvBnReLU2d, \\\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatchNorm2d should be fused into Conv2d before converting to reference module\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_zero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:253\u001b[0m, in \u001b[0;36m_ConvNd.from_reference\u001b[1;34m(cls, ref_qconv, output_scale, output_zero_point)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_reference\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ref_qconv, output_scale, output_zero_point):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Create a (fbgemm/qnnpack) quantized module from a reference quantized module\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m        ref_module (Module): a reference quantized  module, either produced by torch.ao.quantization\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m        output_zero_point (int): zero point for output Tensor\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     qconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     qweight \u001b[38;5;241m=\u001b[39m ref_qconv\u001b[38;5;241m.\u001b[39mget_quantized_weight()\n\u001b[0;32m    266\u001b[0m     qconv\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, ref_qconv\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:78\u001b[0m, in \u001b[0;36mConvReLU2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, out_channels, kernel_size, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     76\u001b[0m              padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m              padding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConvReLU2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:430\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    427\u001b[0m dilation \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# Subclasses of _ConvNd need to call _init rather than __init__. See\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# discussion on PR #49702\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28msuper\u001b[39m(Conv2d, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_init(\n\u001b[0;32m    431\u001b[0m     in_channels, out_channels, kernel_size, stride, padding, dilation,\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m, _pair(\u001b[38;5;241m0\u001b[39m), groups, bias, padding_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:82\u001b[0m, in \u001b[0;36m_ConvNd._init\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     74\u001b[0m qweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_empty_affine_quantized(\n\u001b[0;32m     75\u001b[0m     weight_shape \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kernel_size),\n\u001b[0;32m     76\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, zero_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m factory_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     78\u001b[0m bias_float \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(out_channels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat,\n\u001b[0;32m     80\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m factory_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m}) \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_float\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:439\u001b[0m, in \u001b[0;36mConv2d.set_weight_bias\u001b[1;34m(self, w, b)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_prepack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    443\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::conv2d_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.ao.quantization import QConfig, MinMaxObserver, quantize_fx\n",
    "\n",
    "# Configuration\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # Use 500 classes\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "results = {'batch_size': [], 'accuracy': [], 'time': []}\n",
    "\n",
    "# Static Quantization Setup\n",
    "def prepare_static_quant_model():\n",
    "    # Load model\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).eval()\n",
    "    \n",
    "    # Define quantization configuration\n",
    "    qconfig = QConfig(\n",
    "        activation=MinMaxObserver.with_args(\n",
    "            dtype=torch.quint8,\n",
    "            quant_min=0,\n",
    "            quant_max=255,\n",
    "            reduce_range=True\n",
    "        ),\n",
    "        weight=MinMaxObserver.with_args(\n",
    "            dtype=torch.qint8,\n",
    "            quant_min=-128,\n",
    "            quant_max=127,\n",
    "            qscheme=torch.per_tensor_symmetric\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Prepare model for quantization\n",
    "    model_prepared = quantize_fx.prepare_fx(\n",
    "        model,\n",
    "        {\"\": qconfig},\n",
    "        example_inputs=torch.randn(1, 3, 128, 128)\n",
    "    )\n",
    "    \n",
    "    # Calibrate with sample data\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            model_prepared(torch.randn(1, 3, 128, 128))\n",
    "    \n",
    "    # Convert to quantized model\n",
    "    model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "    return model_quantized\n",
    "\n",
    "# Initialize quantized model\n",
    "print(\"Preparing quantized model...\")\n",
    "model = prepare_static_quant_model()\n",
    "print(\"Static quantization complete!\")\n",
    "\n",
    "# Dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, root, folders, transform=None):\n",
    "        self.root = root\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for class_idx, folder in enumerate(folders):\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            try:\n",
    "                img_file = next(f for f in os.listdir(folder_path) \n",
    "                              if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "                self.image_paths.append((os.path.join(folder_path, img_file), class_idx))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), label\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(160),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_batch(batch_size):\n",
    "    dataset = ImageNetValDataset(val_root, class_folders, transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['time'].append(total_time)\n",
    "    \n",
    "    print(f\"Batch {batch_size:2d} | Acc: {accuracy:.1f}% | Time: {total_time:.1f}s\")\n",
    "\n",
    "# Run evaluation for all batch sizes\n",
    "print(\"\\nStarting evaluation...\")\n",
    "for bs in batch_sizes:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    evaluate_batch(bs)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(results['batch_size'], results['accuracy'], 'bo-', label='Accuracy')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Accuracy (%)', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_ylim(50, 100)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results['batch_size'], results['time'], 'ro-', label='Inference Time')\n",
    "ax2.set_ylabel('Time (seconds)', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Static Quantization: Batch Size Analysis')\n",
    "plt.xticks(results['batch_size'])\n",
    "\n",
    "for bs, acc, t in zip(results['batch_size'], results['accuracy'], results['time']):\n",
    "    ax1.annotate(f'{acc:.1f}%', (bs, acc), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    ax2.annotate(f'{t:.1f}s', (bs, t), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('static_quant_batch_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ab87a6-3959-43ad-b049-ab929333deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX Graph Mode Quantization successful!\n",
      "Output shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b298c00-70ad-41ea-969a-5b30e99d336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp39-cp39-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 7.8/7.8 MB 53.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 62.7 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.57.0 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe100ac-7a91-48a3-8944-0b2ddc3165ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIU856526097\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\prepare.py:1530: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::conv2d_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing quantized model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 128\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_static_quant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatic quantization complete!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mprepare_static_quant_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Use just 1 batch\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Convert to quantized model\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m model_quantized \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_fx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_quantized\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:620\u001b[0m, in \u001b[0;36mconvert_fx\u001b[1;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\" Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    617\u001b[0m \n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    619\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:547\u001b[0m, in \u001b[0;36m_convert_fx\u001b[1;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[0;32m    543\u001b[0m     convert_custom_config \u001b[38;5;241m=\u001b[39m ConvertCustomConfig\u001b[38;5;241m.\u001b[39mfrom_dict(convert_custom_config)\n\u001b[0;32m    545\u001b[0m _check_is_graph_module(graph_module)\n\u001b[1;32m--> 547\u001b[0m quantized \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m preserved_attributes \u001b[38;5;241m=\u001b[39m convert_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr_name \u001b[38;5;129;01min\u001b[39;00m preserved_attributes:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\convert.py:807\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# TODO: maybe move this to quantize_fx.py\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reference:\n\u001b[1;32m--> 807\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mlower_to_fbgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# remove this\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# removes qconfig and activation_post_process modules\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remove_qconfig_flag:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\lower_to_fbgemm.py:16\u001b[0m, in \u001b[0;36mlower_to_fbgemm\u001b[1;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower_to_fbgemm\u001b[39m(\n\u001b[0;32m      9\u001b[0m     model: QuantizedGraphModule,\n\u001b[0;32m     10\u001b[0m     qconfig_map: Dict[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m     11\u001b[0m     node_name_to_scope: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]]\n\u001b[0;32m     12\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuantizedGraphModule:\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    to fbgemm\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lower_to_native_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:928\u001b[0m, in \u001b[0;36m_lower_to_native_backend\u001b[1;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lower_to_native_backend\u001b[39m(\n\u001b[0;32m    920\u001b[0m     model: QuantizedGraphModule,\n\u001b[0;32m    921\u001b[0m     qconfig_map: Dict[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m    922\u001b[0m     node_name_to_scope: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]]\n\u001b[0;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuantizedGraphModule:\n\u001b[0;32m    924\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m    to the native backend in PyTorch (fbgemm/qnnpack), both backends shares the same\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m    operator signature so they can be lowered with the same function\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 928\u001b[0m     \u001b[43m_lower_static_weighted_ref_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    929\u001b[0m     _lower_dynamic_weighted_ref_module(model)\n\u001b[0;32m    930\u001b[0m     _lower_weight_only_weighted_ref_module(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:499\u001b[0m, in \u001b[0;36m_lower_static_weighted_ref_module\u001b[1;34m(model, qconfig_map)\u001b[0m\n\u001b[0;32m    497\u001b[0m output_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, scale_node\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    498\u001b[0m output_zero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, zero_point_node\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[1;32m--> 499\u001b[0m q_module \u001b[38;5;241m=\u001b[39m \u001b[43mq_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_zero_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# replace reference module with quantized module\u001b[39;00m\n\u001b[0;32m    501\u001b[0m parent_name, module_name \u001b[38;5;241m=\u001b[39m _parent_name(ref_node\u001b[38;5;241m.\u001b[39mtarget)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:110\u001b[0m, in \u001b[0;36mConvReLU2d.from_reference\u001b[1;34m(cls, ref_qconv, output_scale, output_zero_point)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_reference\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ref_qconv, output_scale, output_zero_point):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(ref_qconv) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mintrinsic\u001b[38;5;241m.\u001b[39mConvBnReLU2d, \\\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatchNorm2d should be fused into Conv2d before converting to reference module\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_zero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:253\u001b[0m, in \u001b[0;36m_ConvNd.from_reference\u001b[1;34m(cls, ref_qconv, output_scale, output_zero_point)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_reference\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ref_qconv, output_scale, output_zero_point):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Create a (fbgemm/qnnpack) quantized module from a reference quantized module\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m        ref_module (Module): a reference quantized  module, either produced by torch.ao.quantization\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m        output_zero_point (int): zero point for output Tensor\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     qconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_qconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     qweight \u001b[38;5;241m=\u001b[39m ref_qconv\u001b[38;5;241m.\u001b[39mget_quantized_weight()\n\u001b[0;32m    266\u001b[0m     qconv\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, ref_qconv\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:78\u001b[0m, in \u001b[0;36mConvReLU2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, out_channels, kernel_size, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     76\u001b[0m              padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m              padding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConvReLU2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:430\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    427\u001b[0m dilation \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# Subclasses of _ConvNd need to call _init rather than __init__. See\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# discussion on PR #49702\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28msuper\u001b[39m(Conv2d, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_init(\n\u001b[0;32m    431\u001b[0m     in_channels, out_channels, kernel_size, stride, padding, dilation,\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m, _pair(\u001b[38;5;241m0\u001b[39m), groups, bias, padding_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:82\u001b[0m, in \u001b[0;36m_ConvNd._init\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     74\u001b[0m qweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_empty_affine_quantized(\n\u001b[0;32m     75\u001b[0m     weight_shape \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kernel_size),\n\u001b[0;32m     76\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, zero_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m factory_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     78\u001b[0m bias_float \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(out_channels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat,\n\u001b[0;32m     80\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m factory_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m}) \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_float\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:439\u001b[0m, in \u001b[0;36mConv2d.set_weight_bias\u001b[1;34m(self, w, b)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_prepack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    443\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\torch\\_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::conv2d_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.ao.quantization import QConfig, MinMaxObserver, HistogramObserver, quantize_fx\n",
    "\n",
    "# Configuration\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # Use 500 classes\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "results = {'batch_size': [], 'accuracy': [], 'time': []}\n",
    "\n",
    "# Static Quantization with Compatible Configuration\n",
    "def prepare_static_quant_model():\n",
    "    # Load model\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).eval()\n",
    "    \n",
    "    # Use per-tensor quantization for compatibility\n",
    "    qconfig = QConfig(\n",
    "        activation=HistogramObserver.with_args(\n",
    "            dtype=torch.quint8,\n",
    "            reduce_range=False\n",
    "        ),\n",
    "        weight=MinMaxObserver.with_args(\n",
    "            dtype=torch.qint8,\n",
    "            qscheme=torch.per_tensor_symmetric  # Changed from per_channel\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Prepare model for quantization\n",
    "    model_prepared = quantize_fx.prepare_fx(\n",
    "        model,\n",
    "        {\"\": qconfig},\n",
    "        example_inputs=torch.randn(1, 3, 128, 128)\n",
    "    )\n",
    "    \n",
    "    # Calibrate with real data (1 batch)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(160),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    calib_dataset = ImageNetValDataset(val_root, class_folders[:50], transform)\n",
    "    calib_loader = DataLoader(calib_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in calib_loader:\n",
    "            model_prepared(inputs)\n",
    "            break  # Use just 1 batch\n",
    "    \n",
    "    # Convert to quantized model\n",
    "    model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "    \n",
    "    return model_quantized\n",
    "\n",
    "# Dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, root, folders, transform=None):\n",
    "        self.root = root\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for class_idx, folder in enumerate(folders):\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            try:\n",
    "                img_file = next(f for f in os.listdir(folder_path) \n",
    "                              if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "                self.image_paths.append((os.path.join(folder_path, img_file), class_idx))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), label\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_batch(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(160),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = ImageNetValDataset(val_root, class_folders, transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            _ = model(inputs)\n",
    "            break\n",
    "    \n",
    "    # Evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    \n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['time'].append(total_time)\n",
    "    \n",
    "    print(f\"Batch {batch_size:2d} | Acc: {accuracy:.1f}% | Time: {total_time:.2f}s\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Preparing quantized model...\")\n",
    "    model = prepare_static_quant_model()\n",
    "    print(\"Static quantization complete!\\n\")\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    for bs in batch_sizes:\n",
    "        gc.collect()\n",
    "        evaluate_batch(bs)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(results['batch_size'], results['accuracy'], 'bo-', label='Accuracy')\n",
    "    ax1.set_xlabel('Batch Size')\n",
    "    ax1.set_ylabel('Accuracy (%)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_ylim(max(0, min(results['accuracy'])-5), min(100, max(results['accuracy'])+5))\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(results['batch_size'], results['time'], 'ro-', label='Inference Time')\n",
    "    ax2.set_ylabel('Time (seconds)', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\n",
    "\n",
    "    plt.title('Static Quantization Performance Analysis')\n",
    "    plt.xticks(results['batch_size'])\n",
    "\n",
    "    for bs, acc, t in zip(results['batch_size'], results['accuracy'], results['time']):\n",
    "        ax1.annotate(f'{acc:.1f}%', (bs, acc), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "        ax2.annotate(f'{t:.2f}s', (bs, t), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('static_quant_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92400ed5-f58b-4ab2-8377-3adff09953e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 500 folders (1 image each):\n",
      "50/500 | Acc: 50.0% | Time: 2.1s\n",
      "100/500 | Acc: 56.0% | Time: 4.1s\n",
      "150/500 | Acc: 62.0% | Time: 6.1s\n",
      "200/500 | Acc: 59.0% | Time: 8.1s\n",
      "250/500 | Acc: 61.6% | Time: 10.1s\n",
      "300/500 | Acc: 63.3% | Time: 12.6s\n",
      "350/500 | Acc: 63.4% | Time: 14.9s\n",
      "400/500 | Acc: 64.8% | Time: 17.3s\n",
      "450/500 | Acc: 63.3% | Time: 19.3s\n",
      "500/500 | Acc: 62.0% | Time: 21.7s\n",
      "\n",
      "Final Results:\n",
      "- Processed: 500 images\n",
      "- Accuracy: 62.0%\n",
      "- Time: 21.7s (23.0 img/s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# 1. Configure safe quantization\n",
    "torch.backends.quantized.engine = 'none'\n",
    "torch.set_num_threads(2)  # Balanced performance\n",
    "\n",
    "# 2. Load model with safer quantization\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Conv2d},  # Quantize only convolutional layers\n",
    "    dtype=torch.qint8\n",
    ").eval()\n",
    "\n",
    "# 3. Optimized image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(160),  # Better balance than 112/224\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image_safely(folder_path):\n",
    "    try:\n",
    "        img_file = next(f for f in os.listdir(folder_path) \n",
    "                       if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "        img = Image.open(os.path.join(folder_path, img_file)).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# 4. Evaluation with better accuracy\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # Still testing with 500\n",
    "\n",
    "correct = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Evaluating {len(class_folders)} folders (1 image each):\")\n",
    "\n",
    "for class_idx, folder in enumerate(class_folders):\n",
    "    # Memory management\n",
    "    if class_idx % 50 == 0:\n",
    "        gc.collect()\n",
    "    \n",
    "    # Processing\n",
    "    input_tensor = load_image_safely(os.path.join(val_root, folder))\n",
    "    if input_tensor is None:\n",
    "        continue\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        correct += (torch.argmax(output).item() == class_idx)\n",
    "    \n",
    "    # Simplified progress reporting\n",
    "    if (class_idx + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"{class_idx+1}/{len(class_folders)} | \"\n",
    "              f\"Acc: {100*correct/(class_idx+1):.1f}% | \"\n",
    "              f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"- Processed: {len(class_folders)} images\")\n",
    "print(f\"- Accuracy: {100 * correct/len(class_folders):.1f}%\")\n",
    "print(f\"- Time: {total_time:.1f}s ({len(class_folders)/total_time:.1f} img/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f385b015-c459-40c4-92a8-32eb12378bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1 | Acc: 62.0% | Time: 20.0s \n",
      "Batch  4 | Acc: 62.0% | Time: 19.3s \n",
      "Batch  8 | Acc: 62.0% | Time: 18.4s \n",
      "Batch 16 | Acc: 62.0% | Time: 15.9s \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Configuration\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # Use 500 classes\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "results = {'batch_size': [], 'accuracy': [], 'time': [], 'speed': []}\n",
    "\n",
    "# Model setup\n",
    "torch.backends.quantized.engine = 'none'\n",
    "torch.set_num_threads(2)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Conv2d}, dtype=torch.qint8\n",
    ").eval()\n",
    "\n",
    "# Dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, root, folders, transform=None):\n",
    "        self.root = root\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for class_idx, folder in enumerate(folders):\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            try:\n",
    "                img_file = next(f for f in os.listdir(folder_path) \n",
    "                              if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "                self.image_paths.append((os.path.join(folder_path, img_file), class_idx))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), label\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(160),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_batch(batch_size):\n",
    "    dataset = ImageNetValDataset(val_root, class_folders, transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    accuracy = 100 * correct / total\n",
    "    #speed = total / total_time\n",
    "    \n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['time'].append(total_time)\n",
    "   # results['speed'].append(speed)\n",
    "    \n",
    "    print(f\"Batch {batch_size:2d} | Acc: {accuracy:.1f}% | Time: {total_time:.1f}s \")\n",
    "\n",
    "# Run evaluation for all batch sizes\n",
    "for bs in batch_sizes:\n",
    "    gc.collect()\n",
    "    evaluate_batch(bs)\n",
    "\n",
    "# Plot results\n",
    "#plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.plot(results['batch_size'], results['accuracy'], 'bo-')\n",
    "#plt.xlabel('Batch Size')\n",
    "#plt.ylabel('Accuracy (%)')\n",
    "#plt.title('Accuracy vs Batch Size')\n",
    "#plt.grid(True)\n",
    "\n",
    "# Speed plot\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.plot(results['batch_size'], results['time'], 'ro-')\n",
    "#plt.xlabel('Batch Size')\n",
    "#plt.ylabel('time(S)')\n",
    "#plt.title('Inference Time vs Batch Size')\n",
    "#plt.grid(True)\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('batch_size_analysis.png')\n",
    "#plt.show()\n",
    "# Create figure with dual y-axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Accuracy (left axis)\n",
    "ax1 = plt.gca()  # Get current axis\n",
    "ax1.plot(results['batch_size'], results['accuracy'], 'bo-', label='Accuracy')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Accuracy (%)', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_ylim(50, 100)  # Set appropriate accuracy range\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Create second y-axis for Inference Time\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results['batch_size'], results['time'], 'ro-', label='Inference Time')\n",
    "ax2.set_ylabel('Time (seconds)', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Add combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Batch Size Analysis: Accuracy vs Inference Time')\n",
    "plt.xticks(results['batch_size'])  # Show all tested batch sizes\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate data points\n",
    "for bs, acc, t in zip(results['batch_size'], results['accuracy'], results['time']):\n",
    "    ax1.annotate(f'{acc:.1f}%', (bs, acc), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    ax2.annotate(f'{t:.1f}s', (bs, t), textcoords=\"offset points\", xytext=(0,5), ha='center')  # Fixed variable name\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('batch_size_analysis_combined.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "#print(\"\\nFinal Results:\")\n",
    "#for bs, acc, t, spd in zip(results['batch_size'], results['accuracy'], results['time']):\n",
    "#    print(f\"Batch {bs:2d}: {acc:.1f}% accuracy | {t:.1f}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5a303e-7be9-4c01-9b21-76d28fa15841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIU856526097\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1 | Acc: 62.0% | Time: 19.8s \n",
      "Batch  4 | Acc: 62.0% | Time: 19.3s \n",
      "Batch  8 | Acc: 62.0% | Time: 17.8s \n",
      "Batch 16 | Acc: 62.0% | Time: 16.3s \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Configuration\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # Use 500 classes\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "results = {'batch_size': [], 'accuracy': [], 'time': [], 'speed': []}\n",
    "\n",
    "# Model setup\n",
    "torch.backends.quantized.engine = 'none'\n",
    "torch.set_num_threads(2)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "\n",
    "# Dataset class\n",
    "class ImageNetValDataset(Dataset):\n",
    "    def __init__(self, root, folders, transform=None):\n",
    "        self.root = root\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for class_idx, folder in enumerate(folders):\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            try:\n",
    "                img_file = next(f for f in os.listdir(folder_path) \n",
    "                              if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "                self.image_paths.append((os.path.join(folder_path, img_file), class_idx))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), label\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(160),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_batch(batch_size):\n",
    "    dataset = ImageNetValDataset(val_root, class_folders, transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    accuracy = 100 * correct / total\n",
    "    #speed = total / total_time\n",
    "    \n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['time'].append(total_time)\n",
    "   # results['speed'].append(speed)\n",
    "    \n",
    "    print(f\"Batch {batch_size:2d} | Acc: {accuracy:.1f}% | Time: {total_time:.1f}s \")\n",
    "\n",
    "# Run evaluation for all batch sizes\n",
    "for bs in batch_sizes:\n",
    "    gc.collect()\n",
    "    evaluate_batch(bs)\n",
    "\n",
    "# Plot results\n",
    "#plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.plot(results['batch_size'], results['accuracy'], 'bo-')\n",
    "#plt.xlabel('Batch Size')\n",
    "#plt.ylabel('Accuracy (%)')\n",
    "#plt.title('Accuracy vs Batch Size')\n",
    "#plt.grid(True)\n",
    "\n",
    "# Speed plot\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.plot(results['batch_size'], results['time'], 'ro-')\n",
    "#plt.xlabel('Batch Size')\n",
    "#plt.ylabel('time(S)')\n",
    "#plt.title('Inference Time vs Batch Size')\n",
    "#plt.grid(True)\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('batch_size_analysis.png')\n",
    "#plt.show()\n",
    "# Create figure with dual y-axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Accuracy (left axis)\n",
    "ax1 = plt.gca()  # Get current axis\n",
    "ax1.plot(results['batch_size'], results['accuracy'], 'bo-', label='Accuracy')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Accuracy (%)', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_ylim(50, 100)  # Set appropriate accuracy range\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Create second y-axis for Inference Time\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results['batch_size'], results['time'], 'ro-', label='Inference Time')\n",
    "ax2.set_ylabel('Time (seconds)', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Add combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Batch Size Analysis: Accuracy vs Inference Time')\n",
    "plt.xticks(results['batch_size'])  # Show all tested batch sizes\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate data points\n",
    "for bs, acc, t in zip(results['batch_size'], results['accuracy'], results['time']):\n",
    "    ax1.annotate(f'{acc:.1f}%', (bs, acc), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    ax2.annotate(f'{t:.1f}s', (bs, t), textcoords=\"offset points\", xytext=(0,5), ha='center')  # Fixed variable name\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('batch_size_analysis_combined.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "#print(\"\\nFinal Results:\")\n",
    "#for bs, acc, t, spd in zip(results['batch_size'], results['accuracy'], results['time']):\n",
    "#    print(f\"Batch {bs:2d}: {acc:.1f}% accuracy | {t:.1f}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521b4fcf-a2e0-4610-a39c-6612336ed927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIU856526097\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotation file: C:\\Users\\SIU856526097\\datasets\\coco-pose\\annotations\\person_keypoints_val2017.json\n",
      "Selected 32 images for evaluation\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 1\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-11 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 45 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 1 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 75 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 3 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 50 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 4 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 81 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 5 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 25 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 6 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 13 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 7 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 20 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 8 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 65 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 9 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 143 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 10 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 0 detections\n",
      "Processed batch 11 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 135 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 12 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 95 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 13 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 14 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 72 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 15 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 21 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 16 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 17 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 17 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 18 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 22 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 19 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 4 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 20 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 77 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 21 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 57 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 22 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 91 detections\n",
      "After NMS: 11 detections\n",
      "Processed batch 23 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 24 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 25 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 43 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 26 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 27 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 69 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 28 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 48 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 29 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 10 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 30 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 31 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 8 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 32 with 1 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.639\n",
      "mAP@0.5-0.95: 0.639\n",
      "Recall@100: 0.503\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 17.2528s\n",
      "Average Inference Time: 0.5391s per image\n",
      "Images Per Second: 1.85\n",
      "Saved visualizations to visualizations_batch_1 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 4\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-11 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 45 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 75 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 50 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 1 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 81 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 25 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 13 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 20 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 65 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 143 detections\n",
      "After NMS: 12 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 0 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 135 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 3 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 95 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 72 detections\n",
      "After NMS: 7 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 21 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 4 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 17 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 22 detections\n",
      "After NMS: 2 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 4 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 5 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 77 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 57 detections\n",
      "After NMS: 6 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 91 detections\n",
      "After NMS: 11 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 6 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 43 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 7 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 69 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 7 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 48 detections\n",
      "After NMS: 7 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 10 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 5 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 8 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 8 with 4 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.639\n",
      "mAP@0.5-0.95: 0.639\n",
      "Recall@100: 0.503\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 15.9316s\n",
      "Average Inference Time: 0.4979s per image\n",
      "Images Per Second: 2.01\n",
      "Saved visualizations to visualizations_batch_4 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 8\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-11 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 45 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 75 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 50 detections\n",
      "After NMS: 5 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 81 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 25 detections\n",
      "After NMS: 4 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 13 detections\n",
      "After NMS: 1 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 20 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 1 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 65 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 143 detections\n",
      "After NMS: 12 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 0 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 135 detections\n",
      "After NMS: 12 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 95 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 72 detections\n",
      "After NMS: 7 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 21 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 2 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 17 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 22 detections\n",
      "After NMS: 2 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 4 detections\n",
      "After NMS: 2 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 77 detections\n",
      "After NMS: 9 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 57 detections\n",
      "After NMS: 6 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 91 detections\n",
      "After NMS: 11 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 3 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 43 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 7 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 69 detections\n",
      "After NMS: 6 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 48 detections\n",
      "After NMS: 7 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 10 detections\n",
      "After NMS: 2 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 5 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 8 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 4 with 8 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.639\n",
      "mAP@0.5-0.95: 0.639\n",
      "Recall@100: 0.503\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 16.4453s\n",
      "Average Inference Time: 0.5139s per image\n",
      "Images Per Second: 1.95\n",
      "Saved visualizations to visualizations_batch_8 folder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from typing import List, Dict\n",
    "from torchvision.ops import nms\n",
    "# Configuration\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IMG_SIZE = 640\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_PATH = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose\"\n",
    "MAX_IMAGES = 32  # Only process first 32 images\n",
    "BATCH_SIZES = [1, 4, 8]  # Batch sizes to evaluate\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_path, split='val2017', max_images=MAX_IMAGES):\n",
    "        \"\"\"Initialize COCO dataset with limited images\"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.image_dir = os.path.join(root_path, 'images', split)\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        # Check for annotation files\n",
    "        self.annotation_file = os.path.join(root_path, 'annotations', f'person_keypoints_{split}.json')\n",
    "        if not os.path.exists(self.annotation_file):\n",
    "            self.annotation_file = os.path.join(root_path, 'annotations', f'instances_{split}.json')\n",
    "            if not os.path.exists(self.annotation_file):\n",
    "                raise FileNotFoundError(f\"Neither person_keypoints_{split}.json nor instances_{split}.json found\")\n",
    "        \n",
    "        print(f\"Using annotation file: {self.annotation_file}\")\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Create image id to annotations mapping\n",
    "        self.image_info = {img['id']: img for img in self.annotations['images']}\n",
    "        self.annotations_per_image = {}\n",
    "        \n",
    "        for ann in self.annotations['annotations']:\n",
    "            if ann['image_id'] not in self.annotations_per_image:\n",
    "                self.annotations_per_image[ann['image_id']] = []\n",
    "            self.annotations_per_image[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Filter images with annotations and limit to max_images\n",
    "        self.valid_image_ids = [img_id for img_id in self.image_info.keys() \n",
    "                              if img_id in self.annotations_per_image][:max_images]\n",
    "        \n",
    "        print(f\"Selected {len(self.valid_image_ids)} images for evaluation\")\n",
    "        \n",
    "        # Transform for input images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_image_ids[idx]\n",
    "        img_info = self.image_info[img_id]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            return {\n",
    "                'image': torch.zeros((3, IMG_SIZE, IMG_SIZE)),\n",
    "                'original_image': Image.new('RGB', (IMG_SIZE, IMG_SIZE)),\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'original_size': (IMG_SIZE, IMG_SIZE),\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        \n",
    "        original_size = img.size  # (width, height)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annotations = self.annotations_per_image[img_id]\n",
    "        \n",
    "        # Prepare ground truth boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(0)  # 0 is for person class in COCO\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'original_image': img,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': original_size,\n",
    "            'image_id': img_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "from torchvision.ops import nms\n",
    "\n",
    "class YOLOv5Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize YOLOv5 model with evaluation capabilities\"\"\"\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, autoshape=True, force_reload=True)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metric = MeanAveragePrecision(\n",
    "            box_format='xyxy',\n",
    "            iou_type='bbox',\n",
    "            iou_thresholds=[0.5],\n",
    "            rec_thresholds=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True\n",
    "        )\n",
    "\n",
    "    def evaluate_batch(self, batch: Dict) -> List[Dict]:\n",
    "        \"\"\"Evaluate batch of images with raw tensor handling\"\"\"\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        original_sizes = batch['original_size']\n",
    "        image_paths = batch['image_path']\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = self.model(images, size=IMG_SIZE)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Debug\n",
    "        print(f\"Results type: {type(results)}\")\n",
    "        if isinstance(results, torch.Tensor):\n",
    "            print(f\"Raw results shape: {results.shape}\")\n",
    "        else:\n",
    "            print(f\"Results attributes: {dir(results)}\")\n",
    "        \n",
    "        # Ensure results is a tensor\n",
    "        if not isinstance(results, torch.Tensor):\n",
    "            raise ValueError(f\"Expected torch.Tensor, got {type(results)}\")\n",
    "        if results.dim() != 3 or results.shape[0] != batch_size:\n",
    "            raise ValueError(f\"Unexpected results shape: {results.shape}. Expected [batch_size, num_detections, 85]\")\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(batch_size):\n",
    "            pred = results[i]  # [num_detections, 85]\n",
    "            print(f\"Image {i} predictions shape: {pred.shape}\")\n",
    "            \n",
    "            # Extract components\n",
    "            if pred.shape[0] == 0:\n",
    "                boxes = np.zeros((0, 4))\n",
    "                scores = np.zeros(0)\n",
    "                labels = np.zeros(0, dtype=np.int64)\n",
    "            else:\n",
    "                # Convert to xyxy format\n",
    "                x_center = pred[:, 0]\n",
    "                y_center = pred[:, 1]\n",
    "                w = pred[:, 2]\n",
    "                h = pred[:, 3]\n",
    "                conf = pred[:, 4]\n",
    "                class_scores = pred[:, 5:]\n",
    "                \n",
    "                x1 = x_center - w / 2\n",
    "                y1 = y_center - h / 2\n",
    "                x2 = x_center + w / 2\n",
    "                y2 = y_center + h / 2\n",
    "                boxes = torch.stack([x1, y1, x2, y2], dim=1)  # [N, 4]\n",
    "                scores = conf  # [N]\n",
    "                labels = torch.argmax(class_scores, dim=1)  # [N]\n",
    "                \n",
    "                # Apply confidence filter first\n",
    "                mask = scores >= CONFIDENCE_THRESHOLD\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                print(f\"After confidence filter: {boxes.shape[0]} detections\")\n",
    "                \n",
    "                # Apply NMS\n",
    "                if boxes.shape[0] > 0:\n",
    "                    keep = nms(boxes, scores, iou_threshold=0.45)  # IoU threshold for NMS\n",
    "                    boxes = boxes[keep].cpu().numpy()\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = labels[keep].cpu().numpy()\n",
    "                    print(f\"After NMS: {boxes.shape[0]} detections\")\n",
    "                else:\n",
    "                    boxes = np.zeros((0, 4))\n",
    "                    scores = np.zeros(0)\n",
    "                    labels = np.zeros(0, dtype=np.int64)\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            scale_x = orig_w / IMG_SIZE\n",
    "            scale_y = orig_h / IMG_SIZE\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, 0] *= scale_x  # x1\n",
    "                boxes[:, 1] *= scale_y  # y1\n",
    "                boxes[:, 2] *= scale_x  # x2\n",
    "                boxes[:, 3] *= scale_y  # y2\n",
    "            \n",
    "            # Convert to tensors for metrics\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if boxes_tensor.dim() == 1 and len(boxes_tensor) > 0:\n",
    "                boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "            if scores_tensor.dim() == 0 and len(scores_tensor) > 0:\n",
    "                scores_tensor = scores_tensor.unsqueeze(0)\n",
    "            if labels_tensor.dim() == 0 and len(labels_tensor) > 0:\n",
    "                labels_tensor = labels_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Prepare predictions for metrics\n",
    "            pred_metrics = [{\n",
    "                'boxes': boxes_tensor,\n",
    "                'scores': scores_tensor,\n",
    "                'labels': labels_tensor\n",
    "            }]\n",
    "            \n",
    "            # Prepare targets\n",
    "            target_boxes = batch['boxes'][i].cpu().float()\n",
    "            target_labels = batch['labels'][i].cpu().long()\n",
    "            \n",
    "            if target_boxes.dim() == 1 and len(target_boxes) > 0:\n",
    "                target_boxes = target_boxes.unsqueeze(0)\n",
    "            if target_labels.dim() == 0 and len(target_labels) > 0:\n",
    "                target_labels = target_labels.unsqueeze(0)\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': target_boxes,\n",
    "                'labels': target_labels\n",
    "            }]\n",
    "            \n",
    "            # Update metrics\n",
    "            try:\n",
    "                self.metric.update(pred_metrics, targets)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating metrics for image {image_paths[i]}: {e}\")\n",
    "                print(f\"Prediction boxes shape: {boxes_tensor.shape}\")\n",
    "                print(f\"Target boxes shape: {target_boxes.shape}\")\n",
    "                print(f\"Sample prediction boxes: {boxes_tensor[:2]}\")\n",
    "                print(f\"Sample target boxes: {target_boxes[:2]}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image_path': image_paths[i],\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'labels': [self.model.names[int(x)] for x in labels] if len(labels) > 0 else [],\n",
    "                'time': inference_time / batch_size  # Average time per image\n",
    "            })\n",
    "        \n",
    "        return batch_results\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset = COCODataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Please verify the dataset path and files exist\")\n",
    "        raise\n",
    "    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating with batch size: {batch_size}\")\n",
    "        print(f\"Processing {len(dataset)} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=lambda x: x\n",
    "        )\n",
    "        \n",
    "        evaluator = YOLOv5Evaluator()\n",
    "        total_images = 0\n",
    "        total_time = 0\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_dict = {\n",
    "                'image': torch.stack([item['image'] for item in batch]),\n",
    "                'original_image': [item['original_image'] for item in batch],\n",
    "                'boxes': [item['boxes'] for item in batch],\n",
    "                'labels': [item['labels'] for item in batch],\n",
    "                'original_size': [item['original_size'] for item in batch],\n",
    "                'image_id': [item['image_id'] for item in batch],\n",
    "                'image_path': [item['image_path'] for item in batch]\n",
    "            }\n",
    "            \n",
    "            batch_results = evaluator.evaluate_batch(batch_dict)\n",
    "            results.extend(batch_results)\n",
    "            total_images += len(batch_results)\n",
    "            total_time += sum(r['time'] for r in batch_results)\n",
    "            \n",
    "            print(f\"Processed batch {batch_idx+1} with {len(batch)} images\")\n",
    "        \n",
    "        metrics = evaluator.metric.compute()\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"mAP@0.5: {metrics['map_50'].item():.3f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics['map'].item():.3f}\")\n",
    "        print(f\"Recall@100: {metrics['mar_100'].item():.3f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Images Processed: {total_images}\")\n",
    "        print(f\"Total Inference Time: {total_time:.4f}s\")\n",
    "        print(f\"Average Inference Time: {total_time/total_images:.4f}s per image\")\n",
    "        print(f\"Images Per Second: {total_images/total_time:.2f}\")\n",
    "        \n",
    "        # Save visualizations\n",
    "        os.makedirs(f\"visualizations_batch_{batch_size}\", exist_ok=True)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(result['image_path'])\n",
    "                fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                for box, label, score in zip(result['boxes'], result['labels'], result['scores']):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(\n",
    "                        x1, y1-10, \n",
    "                        f\"{label} {score:.2f}\",\n",
    "                        color='white', fontsize=10,\n",
    "                        bbox=dict(facecolor='red', alpha=0.8, pad=2)\n",
    "                    )\n",
    "                \n",
    "                plt.axis('off')\n",
    "                save_path = os.path.join(f\"visualizations_batch_{batch_size}\", f\"result_{idx}.png\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not visualize {result['image_path']}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Saved visualizations to visualizations_batch_{batch_size} folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd911a0b-6b08-4769-b851-945e51c01a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.8-cp39-cp39-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from pycocotools) (3.9.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from pycocotools) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Downloading pycocotools-2.0.8-cp39-cp39-win_amd64.whl (85 kB)\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407c8981-8b84-44db-8ec9-fdbe8e07809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotation file: C:\\Users\\SIU856526097\\datasets\\coco-pose\\annotations\\person_keypoints_val2017.json\n",
      "Selected 32 images for evaluation\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 1\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-11 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 1 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 3 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 4 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 5 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 6 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 7 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 8 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 9 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 10 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 11 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 12 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 13 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 14 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 15 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 16 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 17 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 18 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 19 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 20 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 21 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 22 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 23 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 24 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 25 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 26 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 27 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 28 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 29 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 30 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 31 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 32 with 1 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 39.3777s\n",
      "Average Inference Time: 1.2306s per image\n",
      "Images Per Second: 0.81\n",
      "Saved visualizations to visualizations_batch_1 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 4\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-11 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 1 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 3 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 4 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 5 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 6 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 7 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 8 with 4 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 35.1592s\n",
      "Average Inference Time: 1.0987s per image\n",
      "Images Per Second: 0.91\n",
      "Saved visualizations to visualizations_batch_4 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 8\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-11 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 1 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 3 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 4 with 8 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 33.6561s\n",
      "Average Inference Time: 1.0518s per image\n",
      "Images Per Second: 0.95\n",
      "Saved visualizations to visualizations_batch_8 folder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from typing import List, Dict\n",
    "from torchvision.ops import nms\n",
    "# Configuration\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IMG_SIZE = 640\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_PATH = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose\"\n",
    "MAX_IMAGES = 32  # Only process first 32 images\n",
    "BATCH_SIZES = [1, 4, 8]  # Batch sizes to evaluate\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_path, split='val2017', max_images=MAX_IMAGES):\n",
    "        \"\"\"Initialize COCO dataset with limited images\"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.image_dir = os.path.join(root_path, 'images', split)\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        # Check for annotation files\n",
    "        self.annotation_file = os.path.join(root_path, 'annotations', f'person_keypoints_{split}.json')\n",
    "        if not os.path.exists(self.annotation_file):\n",
    "            self.annotation_file = os.path.join(root_path, 'annotations', f'instances_{split}.json')\n",
    "            if not os.path.exists(self.annotation_file):\n",
    "                raise FileNotFoundError(f\"Neither person_keypoints_{split}.json nor instances_{split}.json found\")\n",
    "        \n",
    "        print(f\"Using annotation file: {self.annotation_file}\")\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Create image id to annotations mapping\n",
    "        self.image_info = {img['id']: img for img in self.annotations['images']}\n",
    "        self.annotations_per_image = {}\n",
    "        \n",
    "        for ann in self.annotations['annotations']:\n",
    "            if ann['image_id'] not in self.annotations_per_image:\n",
    "                self.annotations_per_image[ann['image_id']] = []\n",
    "            self.annotations_per_image[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Filter images with annotations and limit to max_images\n",
    "        self.valid_image_ids = [img_id for img_id in self.image_info.keys() \n",
    "                              if img_id in self.annotations_per_image][:max_images]\n",
    "        \n",
    "        print(f\"Selected {len(self.valid_image_ids)} images for evaluation\")\n",
    "        \n",
    "        # Transform for input images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_image_ids[idx]\n",
    "        img_info = self.image_info[img_id]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            return {\n",
    "                'image': torch.zeros((3, IMG_SIZE, IMG_SIZE)),\n",
    "                'original_image': Image.new('RGB', (IMG_SIZE, IMG_SIZE)),\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'original_size': (IMG_SIZE, IMG_SIZE),\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        \n",
    "        original_size = img.size  # (width, height)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annotations = self.annotations_per_image[img_id]\n",
    "        \n",
    "        # Prepare ground truth boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(0)  # 0 is for person class in COCO\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'original_image': img,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': original_size,\n",
    "            'image_id': img_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "from torchvision.ops import nms\n",
    "\n",
    "class YOLOv5Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize YOLOv5 model with evaluation capabilities\"\"\"\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True, autoshape=True, force_reload=True)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metric = MeanAveragePrecision(\n",
    "            box_format='xyxy',\n",
    "            iou_type='bbox',\n",
    "            iou_thresholds=[0.5],\n",
    "            rec_thresholds=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True\n",
    "        )\n",
    "\n",
    "    def evaluate_batch(self, batch: Dict) -> List[Dict]:\n",
    "        \"\"\"Evaluate batch of images with raw tensor handling\"\"\"\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        original_sizes = batch['original_size']\n",
    "        image_paths = batch['image_path']\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = self.model(images, size=IMG_SIZE)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Debug\n",
    "        print(f\"Results type: {type(results)}\")\n",
    "        if isinstance(results, torch.Tensor):\n",
    "            print(f\"Raw results shape: {results.shape}\")\n",
    "        else:\n",
    "            print(f\"Results attributes: {dir(results)}\")\n",
    "        \n",
    "        # Ensure results is a tensor\n",
    "        if not isinstance(results, torch.Tensor):\n",
    "            raise ValueError(f\"Expected torch.Tensor, got {type(results)}\")\n",
    "        if results.dim() != 3 or results.shape[0] != batch_size:\n",
    "            raise ValueError(f\"Unexpected results shape: {results.shape}. Expected [batch_size, num_detections, 85]\")\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(batch_size):\n",
    "            pred = results[i]  # [num_detections, 85]\n",
    "            print(f\"Image {i} predictions shape: {pred.shape}\")\n",
    "            \n",
    "            # Extract components\n",
    "            if pred.shape[0] == 0:\n",
    "                boxes = np.zeros((0, 4))\n",
    "                scores = np.zeros(0)\n",
    "                labels = np.zeros(0, dtype=np.int64)\n",
    "            else:\n",
    "                # Convert to xyxy format\n",
    "                x_center = pred[:, 0]\n",
    "                y_center = pred[:, 1]\n",
    "                w = pred[:, 2]\n",
    "                h = pred[:, 3]\n",
    "                conf = pred[:, 4]\n",
    "                class_scores = pred[:, 5:]\n",
    "                \n",
    "                x1 = x_center - w / 2\n",
    "                y1 = y_center - h / 2\n",
    "                x2 = x_center + w / 2\n",
    "                y2 = y_center + h / 2\n",
    "                boxes = torch.stack([x1, y1, x2, y2], dim=1)  # [N, 4]\n",
    "                scores = conf  # [N]\n",
    "                labels = torch.argmax(class_scores, dim=1)  # [N]\n",
    "                \n",
    "                # Apply confidence filter first\n",
    "                mask = scores >= CONFIDENCE_THRESHOLD\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                print(f\"After confidence filter: {boxes.shape[0]} detections\")\n",
    "                \n",
    "                # Apply NMS\n",
    "                if boxes.shape[0] > 0:\n",
    "                    keep = nms(boxes, scores, iou_threshold=0.45)  # IoU threshold for NMS\n",
    "                    boxes = boxes[keep].cpu().numpy()\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = labels[keep].cpu().numpy()\n",
    "                    print(f\"After NMS: {boxes.shape[0]} detections\")\n",
    "                else:\n",
    "                    boxes = np.zeros((0, 4))\n",
    "                    scores = np.zeros(0)\n",
    "                    labels = np.zeros(0, dtype=np.int64)\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            scale_x = orig_w / IMG_SIZE\n",
    "            scale_y = orig_h / IMG_SIZE\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, 0] *= scale_x  # x1\n",
    "                boxes[:, 1] *= scale_y  # y1\n",
    "                boxes[:, 2] *= scale_x  # x2\n",
    "                boxes[:, 3] *= scale_y  # y2\n",
    "            \n",
    "            # Convert to tensors for metrics\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if boxes_tensor.dim() == 1 and len(boxes_tensor) > 0:\n",
    "                boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "            if scores_tensor.dim() == 0 and len(scores_tensor) > 0:\n",
    "                scores_tensor = scores_tensor.unsqueeze(0)\n",
    "            if labels_tensor.dim() == 0 and len(labels_tensor) > 0:\n",
    "                labels_tensor = labels_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Prepare predictions for metrics\n",
    "            pred_metrics = [{\n",
    "                'boxes': boxes_tensor,\n",
    "                'scores': scores_tensor,\n",
    "                'labels': labels_tensor\n",
    "            }]\n",
    "            \n",
    "            # Prepare targets\n",
    "            target_boxes = batch['boxes'][i].cpu().float()\n",
    "            target_labels = batch['labels'][i].cpu().long()\n",
    "            \n",
    "            if target_boxes.dim() == 1 and len(target_boxes) > 0:\n",
    "                target_boxes = target_boxes.unsqueeze(0)\n",
    "            if target_labels.dim() == 0 and len(target_labels) > 0:\n",
    "                target_labels = target_labels.unsqueeze(0)\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': target_boxes,\n",
    "                'labels': target_labels\n",
    "            }]\n",
    "            \n",
    "            # Update metrics\n",
    "            try:\n",
    "                self.metric.update(pred_metrics, targets)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating metrics for image {image_paths[i]}: {e}\")\n",
    "                print(f\"Prediction boxes shape: {boxes_tensor.shape}\")\n",
    "                print(f\"Target boxes shape: {target_boxes.shape}\")\n",
    "                print(f\"Sample prediction boxes: {boxes_tensor[:2]}\")\n",
    "                print(f\"Sample target boxes: {target_boxes[:2]}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image_path': image_paths[i],\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'labels': [self.model.names[int(x)] for x in labels] if len(labels) > 0 else [],\n",
    "                'time': inference_time / batch_size  # Average time per image\n",
    "            })\n",
    "        \n",
    "        return batch_results\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset = COCODataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Please verify the dataset path and files exist\")\n",
    "        raise\n",
    "    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating with batch size: {batch_size}\")\n",
    "        print(f\"Processing {len(dataset)} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=lambda x: x\n",
    "        )\n",
    "        \n",
    "        evaluator = YOLOv5Evaluator()\n",
    "        total_images = 0\n",
    "        total_time = 0\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_dict = {\n",
    "                'image': torch.stack([item['image'] for item in batch]),\n",
    "                'original_image': [item['original_image'] for item in batch],\n",
    "                'boxes': [item['boxes'] for item in batch],\n",
    "                'labels': [item['labels'] for item in batch],\n",
    "                'original_size': [item['original_size'] for item in batch],\n",
    "                'image_id': [item['image_id'] for item in batch],\n",
    "                'image_path': [item['image_path'] for item in batch]\n",
    "            }\n",
    "            \n",
    "            batch_results = evaluator.evaluate_batch(batch_dict)\n",
    "            results.extend(batch_results)\n",
    "            total_images += len(batch_results)\n",
    "            total_time += sum(r['time'] for r in batch_results)\n",
    "            \n",
    "            print(f\"Processed batch {batch_idx+1} with {len(batch)} images\")\n",
    "        \n",
    "        metrics = evaluator.metric.compute()\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"mAP@0.5: {metrics['map_50'].item():.3f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics['map'].item():.3f}\")\n",
    "        print(f\"Recall@100: {metrics['mar_100'].item():.3f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Images Processed: {total_images}\")\n",
    "        print(f\"Total Inference Time: {total_time:.4f}s\")\n",
    "        print(f\"Average Inference Time: {total_time/total_images:.4f}s per image\")\n",
    "        print(f\"Images Per Second: {total_images/total_time:.2f}\")\n",
    "        \n",
    "        # Save visualizations\n",
    "        os.makedirs(f\"visualizations_batch_{batch_size}\", exist_ok=True)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(result['image_path'])\n",
    "                fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                for box, label, score in zip(result['boxes'], result['labels'], result['scores']):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(\n",
    "                        x1, y1-10, \n",
    "                        f\"{label} {score:.2f}\",\n",
    "                        color='white', fontsize=10,\n",
    "                        bbox=dict(facecolor='red', alpha=0.8, pad=2)\n",
    "                    )\n",
    "                \n",
    "                plt.axis('off')\n",
    "                save_path = os.path.join(f\"visualizations_batch_{batch_size}\", f\"result_{idx}.png\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not visualize {result['image_path']}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Saved visualizations to visualizations_batch_{batch_size} folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07221139-05d3-4030-9ff2-c3fae3d44bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 500 folders (1 image each):\n",
      "50/500 | Acc: 42.0% | Time: 1.7s\n",
      "100/500 | Acc: 45.0% | Time: 2.9s\n",
      "150/500 | Acc: 52.0% | Time: 4.1s\n",
      "200/500 | Acc: 48.5% | Time: 5.3s\n",
      "250/500 | Acc: 51.6% | Time: 6.5s\n",
      "300/500 | Acc: 52.3% | Time: 7.8s\n",
      "350/500 | Acc: 52.9% | Time: 9.0s\n",
      "400/500 | Acc: 52.2% | Time: 10.3s\n",
      "450/500 | Acc: 52.2% | Time: 11.5s\n",
      "500/500 | Acc: 51.2% | Time: 12.5s\n",
      "\n",
      "Final Results:\n",
      "- Processed: 500 images\n",
      "- Accuracy: 51.2%\n",
      "- Time: 12.5s (40.0 img/s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# 1. Configure safe quantization\n",
    "torch.backends.quantized.engine = 'none'\n",
    "torch.set_num_threads(2)  # Balanced performance\n",
    "\n",
    "# 2. Load model with safer quantization\n",
    "model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Conv2d},  # Quantize only convolutional layers\n",
    "    dtype=torch.qint8\n",
    ").eval()\n",
    "\n",
    "# 3. Optimized image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(160),  # Better balance than 112/224\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image_safely(folder_path):\n",
    "    try:\n",
    "        img_file = next(f for f in os.listdir(folder_path) \n",
    "                       if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "        img = Image.open(os.path.join(folder_path, img_file)).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# 4. Evaluation with better accuracy\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # Still testing with 500\n",
    "\n",
    "correct = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Evaluating {len(class_folders)} folders (1 image each):\")\n",
    "\n",
    "for class_idx, folder in enumerate(class_folders):\n",
    "    # Memory management\n",
    "    if class_idx % 50 == 0:\n",
    "        gc.collect()\n",
    "    \n",
    "    # Processing\n",
    "    input_tensor = load_image_safely(os.path.join(val_root, folder))\n",
    "    if input_tensor is None:\n",
    "        continue\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        correct += (torch.argmax(output).item() == class_idx)\n",
    "    \n",
    "    # Simplified progress reporting\n",
    "    if (class_idx + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"{class_idx+1}/{len(class_folders)} | \"\n",
    "              f\"Acc: {100*correct/(class_idx+1):.1f}% | \"\n",
    "              f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"- Processed: {len(class_folders)} images\")\n",
    "print(f\"- Accuracy: {100 * correct/len(class_folders):.1f}%\")\n",
    "print(f\"- Time: {total_time:.1f}s ({len(class_folders)/total_time:.1f} img/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4658d580-30ba-4473-b09c-1be6ca258705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 32 images with person annotations.\n",
      "Created subset at C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\n",
      "Copied 32 images\n",
      "Included 161 person annotations\n",
      "Annotation file saved at C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\\annotations\\instances_val2017.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "original_dataset_path = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose\"\n",
    "subset_dataset_path = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\"\n",
    "original_image_dir = os.path.join(original_dataset_path, \"images\", \"val2017\")\n",
    "original_annotation_file = os.path.join(original_dataset_path, \"annotations\", \"instances_val2017.json\")\n",
    "subset_image_dir = os.path.join(subset_dataset_path, \"images\", \"val2017\")\n",
    "subset_annotation_dir = os.path.join(subset_dataset_path, \"annotations\")\n",
    "subset_annotation_file = os.path.join(subset_annotation_dir, \"instances_val2017.json\")\n",
    "\n",
    "# Number of images to include\n",
    "max_images = 32\n",
    "\n",
    "def create_subset():\n",
    "    # Create subset directory structure\n",
    "    os.makedirs(subset_image_dir, exist_ok=True)\n",
    "    os.makedirs(subset_annotation_dir, exist_ok=True)\n",
    "\n",
    "    # Load original annotations\n",
    "    with open(original_annotation_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Filter images with person annotations\n",
    "    image_ids_with_person = set()\n",
    "    for ann in coco_data['annotations']:\n",
    "        if ann['category_id'] == 1:  # COCO category_id 1 is 'person'\n",
    "            image_ids_with_person.add(ann['image_id'])\n",
    "\n",
    "    # Get image info for images with person annotations\n",
    "    valid_images = [img for img in coco_data['images'] if img['id'] in image_ids_with_person]\n",
    "    selected_images = valid_images[:max_images]\n",
    "\n",
    "    if len(selected_images) < max_images:\n",
    "        print(f\"Warning: Only found {len(selected_images)} images with person annotations.\")\n",
    "    else:\n",
    "        print(f\"Selected {len(selected_images)} images with person annotations.\")\n",
    "\n",
    "    # Copy selected images to subset directory\n",
    "    selected_image_ids = set()\n",
    "    for img_info in selected_images:\n",
    "        src_path = os.path.join(original_image_dir, img_info['file_name'])\n",
    "        dst_path = os.path.join(subset_image_dir, img_info['file_name'])\n",
    "        try:\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            selected_image_ids.add(img_info['id'])\n",
    "            # Verify image is valid\n",
    "            Image.open(dst_path).verify()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to copy or verify image {img_info['file_name']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Filter annotations for selected images\n",
    "    subset_annotations = [\n",
    "        ann for ann in coco_data['annotations']\n",
    "        if ann['image_id'] in selected_image_ids and ann['category_id'] == 1\n",
    "    ]\n",
    "\n",
    "    # Create subset annotation JSON\n",
    "    subset_data = {\n",
    "        'images': [img for img in coco_data['images'] if img['id'] in selected_image_ids],\n",
    "        'annotations': subset_annotations,\n",
    "        'categories': [cat for cat in coco_data['categories'] if cat['id'] == 1],  # Only 'person'\n",
    "        'info': coco_data.get('info', {}),\n",
    "        'licenses': coco_data.get('licenses', [])\n",
    "    }\n",
    "\n",
    "    # Save subset annotations\n",
    "    with open(subset_annotation_file, 'w') as f:\n",
    "        json.dump(subset_data, f, indent=4)\n",
    "\n",
    "    print(f\"Created subset at {subset_dataset_path}\")\n",
    "    print(f\"Copied {len(subset_data['images'])} images\")\n",
    "    print(f\"Included {len(subset_data['annotations'])} person annotations\")\n",
    "    print(f\"Annotation file saved at {subset_annotation_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify original dataset exists\n",
    "    if not os.path.exists(original_dataset_path):\n",
    "        print(f\"Original dataset not found at {original_dataset_path}\")\n",
    "    elif not os.path.exists(original_image_dir):\n",
    "        print(f\"Image directory not found at {original_image_dir}\")\n",
    "    elif not os.path.exists(original_annotation_file):\n",
    "        print(f\"Annotation file not found at {original_annotation_file}\")\n",
    "    else:\n",
    "        create_subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c64d35f-fc6d-4821-954e-103b21629842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIU856526097\\AppData\\Local\\anaconda3\\envs\\torch_quant\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotation file: C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\\annotations\\instances_val2017.json\n",
      "Selected 32 images for evaluation\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 1\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 1 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 3 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 4 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 5 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 6 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 7 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 8 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 9 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 10 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 11 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 12 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 13 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 14 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 15 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 16 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 17 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 18 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 19 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 20 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 21 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 22 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 23 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 24 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 25 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 26 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 27 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 28 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 29 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 30 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 31 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 32 with 1 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 39.9557s\n",
      "Average Inference Time: 1.2486s per image\n",
      "Images Per Second: 0.80\n",
      "Saved visualizations to visualizations_batch_1 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 4\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 1 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 3 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 4 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 5 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 6 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 7 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 8 with 4 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 35.6984s\n",
      "Average Inference Time: 1.1156s per image\n",
      "Images Per Second: 0.90\n",
      "Saved visualizations to visualizations_batch_4 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 8\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 1 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 3 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 4 with 8 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 36.1588s\n",
      "Average Inference Time: 1.1300s per image\n",
      "Images Per Second: 0.88\n",
      "Saved visualizations to visualizations_batch_8 folder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from typing import List, Dict\n",
    "from torchvision.ops import nms\n",
    "# Configuration\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IMG_SIZE = 640\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_PATH = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\"\n",
    "MAX_IMAGES = 32  # Only process first 32 images\n",
    "BATCH_SIZES = [1, 4, 8]  # Batch sizes to evaluate\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_path, split='val2017', max_images=MAX_IMAGES):\n",
    "        \"\"\"Initialize COCO dataset with limited images\"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.image_dir = os.path.join(root_path, 'images', split)\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        # Check for annotation files\n",
    "        self.annotation_file = os.path.join(root_path, 'annotations', f'person_keypoints_{split}.json')\n",
    "        if not os.path.exists(self.annotation_file):\n",
    "            self.annotation_file = os.path.join(root_path, 'annotations', f'instances_{split}.json')\n",
    "            if not os.path.exists(self.annotation_file):\n",
    "                raise FileNotFoundError(f\"Neither person_keypoints_{split}.json nor instances_{split}.json found\")\n",
    "        \n",
    "        print(f\"Using annotation file: {self.annotation_file}\")\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Create image id to annotations mapping\n",
    "        self.image_info = {img['id']: img for img in self.annotations['images']}\n",
    "        self.annotations_per_image = {}\n",
    "        \n",
    "        for ann in self.annotations['annotations']:\n",
    "            if ann['image_id'] not in self.annotations_per_image:\n",
    "                self.annotations_per_image[ann['image_id']] = []\n",
    "            self.annotations_per_image[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Filter images with annotations and limit to max_images\n",
    "        self.valid_image_ids = [img_id for img_id in self.image_info.keys() \n",
    "                              if img_id in self.annotations_per_image][:max_images]\n",
    "        \n",
    "        print(f\"Selected {len(self.valid_image_ids)} images for evaluation\")\n",
    "        \n",
    "        # Transform for input images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_image_ids[idx]\n",
    "        img_info = self.image_info[img_id]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            return {\n",
    "                'image': torch.zeros((3, IMG_SIZE, IMG_SIZE)),\n",
    "                'original_image': Image.new('RGB', (IMG_SIZE, IMG_SIZE)),\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'original_size': (IMG_SIZE, IMG_SIZE),\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        \n",
    "        original_size = img.size  # (width, height)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annotations = self.annotations_per_image[img_id]\n",
    "        \n",
    "        # Prepare ground truth boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(0)  # 0 is for person class in COCO\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'original_image': img,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': original_size,\n",
    "            'image_id': img_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "from torchvision.ops import nms\n",
    "\n",
    "class YOLOv5Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize YOLOv5 model with evaluation capabilities\"\"\"\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True, autoshape=True, force_reload=True)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metric = MeanAveragePrecision(\n",
    "            box_format='xyxy',\n",
    "            iou_type='bbox',\n",
    "            iou_thresholds=[0.5],\n",
    "            rec_thresholds=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True\n",
    "        )\n",
    "\n",
    "    def evaluate_batch(self, batch: Dict) -> List[Dict]:\n",
    "        \"\"\"Evaluate batch of images with raw tensor handling\"\"\"\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        original_sizes = batch['original_size']\n",
    "        image_paths = batch['image_path']\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = self.model(images, size=IMG_SIZE)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Debug\n",
    "        print(f\"Results type: {type(results)}\")\n",
    "        if isinstance(results, torch.Tensor):\n",
    "            print(f\"Raw results shape: {results.shape}\")\n",
    "        else:\n",
    "            print(f\"Results attributes: {dir(results)}\")\n",
    "        \n",
    "        # Ensure results is a tensor\n",
    "        if not isinstance(results, torch.Tensor):\n",
    "            raise ValueError(f\"Expected torch.Tensor, got {type(results)}\")\n",
    "        if results.dim() != 3 or results.shape[0] != batch_size:\n",
    "            raise ValueError(f\"Unexpected results shape: {results.shape}. Expected [batch_size, num_detections, 85]\")\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(batch_size):\n",
    "            pred = results[i]  # [num_detections, 85]\n",
    "            print(f\"Image {i} predictions shape: {pred.shape}\")\n",
    "            \n",
    "            # Extract components\n",
    "            if pred.shape[0] == 0:\n",
    "                boxes = np.zeros((0, 4))\n",
    "                scores = np.zeros(0)\n",
    "                labels = np.zeros(0, dtype=np.int64)\n",
    "            else:\n",
    "                # Convert to xyxy format\n",
    "                x_center = pred[:, 0]\n",
    "                y_center = pred[:, 1]\n",
    "                w = pred[:, 2]\n",
    "                h = pred[:, 3]\n",
    "                conf = pred[:, 4]\n",
    "                class_scores = pred[:, 5:]\n",
    "                \n",
    "                x1 = x_center - w / 2\n",
    "                y1 = y_center - h / 2\n",
    "                x2 = x_center + w / 2\n",
    "                y2 = y_center + h / 2\n",
    "                boxes = torch.stack([x1, y1, x2, y2], dim=1)  # [N, 4]\n",
    "                scores = conf  # [N]\n",
    "                labels = torch.argmax(class_scores, dim=1)  # [N]\n",
    "                \n",
    "                # Apply confidence filter first\n",
    "                mask = scores >= CONFIDENCE_THRESHOLD\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                print(f\"After confidence filter: {boxes.shape[0]} detections\")\n",
    "                \n",
    "                # Apply NMS\n",
    "                if boxes.shape[0] > 0:\n",
    "                    keep = nms(boxes, scores, iou_threshold=0.45)  # IoU threshold for NMS\n",
    "                    boxes = boxes[keep].cpu().numpy()\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = labels[keep].cpu().numpy()\n",
    "                    print(f\"After NMS: {boxes.shape[0]} detections\")\n",
    "                else:\n",
    "                    boxes = np.zeros((0, 4))\n",
    "                    scores = np.zeros(0)\n",
    "                    labels = np.zeros(0, dtype=np.int64)\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            scale_x = orig_w / IMG_SIZE\n",
    "            scale_y = orig_h / IMG_SIZE\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, 0] *= scale_x  # x1\n",
    "                boxes[:, 1] *= scale_y  # y1\n",
    "                boxes[:, 2] *= scale_x  # x2\n",
    "                boxes[:, 3] *= scale_y  # y2\n",
    "            \n",
    "            # Convert to tensors for metrics\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if boxes_tensor.dim() == 1 and len(boxes_tensor) > 0:\n",
    "                boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "            if scores_tensor.dim() == 0 and len(scores_tensor) > 0:\n",
    "                scores_tensor = scores_tensor.unsqueeze(0)\n",
    "            if labels_tensor.dim() == 0 and len(labels_tensor) > 0:\n",
    "                labels_tensor = labels_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Prepare predictions for metrics\n",
    "            pred_metrics = [{\n",
    "                'boxes': boxes_tensor,\n",
    "                'scores': scores_tensor,\n",
    "                'labels': labels_tensor\n",
    "            }]\n",
    "            \n",
    "            # Prepare targets\n",
    "            target_boxes = batch['boxes'][i].cpu().float()\n",
    "            target_labels = batch['labels'][i].cpu().long()\n",
    "            \n",
    "            if target_boxes.dim() == 1 and len(target_boxes) > 0:\n",
    "                target_boxes = target_boxes.unsqueeze(0)\n",
    "            if target_labels.dim() == 0 and len(target_labels) > 0:\n",
    "                target_labels = target_labels.unsqueeze(0)\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': target_boxes,\n",
    "                'labels': target_labels\n",
    "            }]\n",
    "            \n",
    "            # Update metrics\n",
    "            try:\n",
    "                self.metric.update(pred_metrics, targets)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating metrics for image {image_paths[i]}: {e}\")\n",
    "                print(f\"Prediction boxes shape: {boxes_tensor.shape}\")\n",
    "                print(f\"Target boxes shape: {target_boxes.shape}\")\n",
    "                print(f\"Sample prediction boxes: {boxes_tensor[:2]}\")\n",
    "                print(f\"Sample target boxes: {target_boxes[:2]}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image_path': image_paths[i],\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'labels': [self.model.names[int(x)] for x in labels] if len(labels) > 0 else [],\n",
    "                'time': inference_time / batch_size  # Average time per image\n",
    "            })\n",
    "        \n",
    "        return batch_results\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset = COCODataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Please verify the dataset path and files exist\")\n",
    "        raise\n",
    "    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating with batch size: {batch_size}\")\n",
    "        print(f\"Processing {len(dataset)} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=lambda x: x\n",
    "        )\n",
    "        \n",
    "        evaluator = YOLOv5Evaluator()\n",
    "        total_images = 0\n",
    "        total_time = 0\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_dict = {\n",
    "                'image': torch.stack([item['image'] for item in batch]),\n",
    "                'original_image': [item['original_image'] for item in batch],\n",
    "                'boxes': [item['boxes'] for item in batch],\n",
    "                'labels': [item['labels'] for item in batch],\n",
    "                'original_size': [item['original_size'] for item in batch],\n",
    "                'image_id': [item['image_id'] for item in batch],\n",
    "                'image_path': [item['image_path'] for item in batch]\n",
    "            }\n",
    "            \n",
    "            batch_results = evaluator.evaluate_batch(batch_dict)\n",
    "            results.extend(batch_results)\n",
    "            total_images += len(batch_results)\n",
    "            total_time += sum(r['time'] for r in batch_results)\n",
    "            \n",
    "            print(f\"Processed batch {batch_idx+1} with {len(batch)} images\")\n",
    "        \n",
    "        metrics = evaluator.metric.compute()\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"mAP@0.5: {metrics['map_50'].item():.3f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics['map'].item():.3f}\")\n",
    "        print(f\"Recall@100: {metrics['mar_100'].item():.3f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Images Processed: {total_images}\")\n",
    "        print(f\"Total Inference Time: {total_time:.4f}s\")\n",
    "        print(f\"Average Inference Time: {total_time/total_images:.4f}s per image\")\n",
    "        #print(f\"Images Per Second: {total_images/total_time:.2f}\")\n",
    "        \n",
    "        # Save visualizations\n",
    "        os.makedirs(f\"visualizations_batch_{batch_size}\", exist_ok=True)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(result['image_path'])\n",
    "                fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                for box, label, score in zip(result['boxes'], result['labels'], result['scores']):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(\n",
    "                        x1, y1-10, \n",
    "                        f\"{label} {score:.2f}\",\n",
    "                        color='white', fontsize=10,\n",
    "                        bbox=dict(facecolor='red', alpha=0.8, pad=2)\n",
    "                    )\n",
    "                \n",
    "                plt.axis('off')\n",
    "                save_path = os.path.join(f\"visualizations_batch_{batch_size}\", f\"result_{idx}.png\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not visualize {result['image_path']}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Saved visualizations to visualizations_batch_{batch_size} folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf526249-d5fb-4298-9b3d-c4148f87373f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
