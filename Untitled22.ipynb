{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f152edc-2ecf-4473-a7c4-9bc8fd454e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIU856526097\\AppData\\Local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotation file: C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\\annotations\\instances_val2017.json\n",
      "Selected 32 images for evaluation\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 1\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 1 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 3 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 4 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 5 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 6 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 7 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 8 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 9 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 10 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 11 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 12 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 13 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 14 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 15 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 16 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 17 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 18 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 19 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 20 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 21 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 22 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 23 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 24 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 25 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 26 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 27 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 28 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 29 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 30 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 31 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 32 with 1 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 40.0796s\n",
      "Average Inference Time: 1.2525s per image\n",
      "Saved visualizations to visualizations_batch_1 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 4\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 1 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 3 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 4 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 5 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 6 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 7 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 8 with 4 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 40.4321s\n",
      "Average Inference Time: 1.2635s per image\n",
      "Saved visualizations to visualizations_batch_4 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 8\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 1 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 3 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 4 with 8 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 34.5726s\n",
      "Average Inference Time: 1.0804s per image\n",
      "Saved visualizations to visualizations_batch_8 folder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from typing import List, Dict\n",
    "from torchvision.ops import nms\n",
    "# Configuration\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IMG_SIZE = 640\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_PATH = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\"\n",
    "MAX_IMAGES = 32  # Only process first 32 images\n",
    "BATCH_SIZES = [1, 4, 8]  # Batch sizes to evaluate\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_path, split='val2017', max_images=MAX_IMAGES):\n",
    "        \"\"\"Initialize COCO dataset with limited images\"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.image_dir = os.path.join(root_path, 'images', split)\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        # Check for annotation files\n",
    "        self.annotation_file = os.path.join(root_path, 'annotations', f'person_keypoints_{split}.json')\n",
    "        if not os.path.exists(self.annotation_file):\n",
    "            self.annotation_file = os.path.join(root_path, 'annotations', f'instances_{split}.json')\n",
    "            if not os.path.exists(self.annotation_file):\n",
    "                raise FileNotFoundError(f\"Neither person_keypoints_{split}.json nor instances_{split}.json found\")\n",
    "        \n",
    "        print(f\"Using annotation file: {self.annotation_file}\")\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Create image id to annotations mapping\n",
    "        self.image_info = {img['id']: img for img in self.annotations['images']}\n",
    "        self.annotations_per_image = {}\n",
    "        \n",
    "        for ann in self.annotations['annotations']:\n",
    "            if ann['image_id'] not in self.annotations_per_image:\n",
    "                self.annotations_per_image[ann['image_id']] = []\n",
    "            self.annotations_per_image[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Filter images with annotations and limit to max_images\n",
    "        self.valid_image_ids = [img_id for img_id in self.image_info.keys() \n",
    "                              if img_id in self.annotations_per_image][:max_images]\n",
    "        \n",
    "        print(f\"Selected {len(self.valid_image_ids)} images for evaluation\")\n",
    "        \n",
    "        # Transform for input images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_image_ids[idx]\n",
    "        img_info = self.image_info[img_id]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            return {\n",
    "                'image': torch.zeros((3, IMG_SIZE, IMG_SIZE)),\n",
    "                'original_image': Image.new('RGB', (IMG_SIZE, IMG_SIZE)),\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'original_size': (IMG_SIZE, IMG_SIZE),\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        \n",
    "        original_size = img.size  # (width, height)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annotations = self.annotations_per_image[img_id]\n",
    "        \n",
    "        # Prepare ground truth boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(0)  # 0 is for person class in COCO\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'original_image': img,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': original_size,\n",
    "            'image_id': img_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "from torchvision.ops import nms\n",
    "\n",
    "class YOLOv5Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize YOLOv5 model with evaluation capabilities\"\"\"\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True, autoshape=True, force_reload=True)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metric = MeanAveragePrecision(\n",
    "            box_format='xyxy',\n",
    "            iou_type='bbox',\n",
    "            iou_thresholds=[0.5],\n",
    "            rec_thresholds=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True\n",
    "        )\n",
    "\n",
    "    def evaluate_batch(self, batch: Dict) -> List[Dict]:\n",
    "        \"\"\"Evaluate batch of images with raw tensor handling\"\"\"\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        original_sizes = batch['original_size']\n",
    "        image_paths = batch['image_path']\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = self.model(images, size=IMG_SIZE)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Debug\n",
    "        print(f\"Results type: {type(results)}\")\n",
    "        if isinstance(results, torch.Tensor):\n",
    "            print(f\"Raw results shape: {results.shape}\")\n",
    "        else:\n",
    "            print(f\"Results attributes: {dir(results)}\")\n",
    "        \n",
    "        # Ensure results is a tensor\n",
    "        if not isinstance(results, torch.Tensor):\n",
    "            raise ValueError(f\"Expected torch.Tensor, got {type(results)}\")\n",
    "        if results.dim() != 3 or results.shape[0] != batch_size:\n",
    "            raise ValueError(f\"Unexpected results shape: {results.shape}. Expected [batch_size, num_detections, 85]\")\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(batch_size):\n",
    "            pred = results[i]  # [num_detections, 85]\n",
    "            print(f\"Image {i} predictions shape: {pred.shape}\")\n",
    "            \n",
    "            # Extract components\n",
    "            if pred.shape[0] == 0:\n",
    "                boxes = np.zeros((0, 4))\n",
    "                scores = np.zeros(0)\n",
    "                labels = np.zeros(0, dtype=np.int64)\n",
    "            else:\n",
    "                # Convert to xyxy format\n",
    "                x_center = pred[:, 0]\n",
    "                y_center = pred[:, 1]\n",
    "                w = pred[:, 2]\n",
    "                h = pred[:, 3]\n",
    "                conf = pred[:, 4]\n",
    "                class_scores = pred[:, 5:]\n",
    "                \n",
    "                x1 = x_center - w / 2\n",
    "                y1 = y_center - h / 2\n",
    "                x2 = x_center + w / 2\n",
    "                y2 = y_center + h / 2\n",
    "                boxes = torch.stack([x1, y1, x2, y2], dim=1)  # [N, 4]\n",
    "                scores = conf  # [N]\n",
    "                labels = torch.argmax(class_scores, dim=1)  # [N]\n",
    "                \n",
    "                # Apply confidence filter first\n",
    "                mask = scores >= CONFIDENCE_THRESHOLD\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                print(f\"After confidence filter: {boxes.shape[0]} detections\")\n",
    "                \n",
    "                # Apply NMS\n",
    "                if boxes.shape[0] > 0:\n",
    "                    keep = nms(boxes, scores, iou_threshold=0.45)  # IoU threshold for NMS\n",
    "                    boxes = boxes[keep].cpu().numpy()\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = labels[keep].cpu().numpy()\n",
    "                    print(f\"After NMS: {boxes.shape[0]} detections\")\n",
    "                else:\n",
    "                    boxes = np.zeros((0, 4))\n",
    "                    scores = np.zeros(0)\n",
    "                    labels = np.zeros(0, dtype=np.int64)\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            scale_x = orig_w / IMG_SIZE\n",
    "            scale_y = orig_h / IMG_SIZE\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, 0] *= scale_x  # x1\n",
    "                boxes[:, 1] *= scale_y  # y1\n",
    "                boxes[:, 2] *= scale_x  # x2\n",
    "                boxes[:, 3] *= scale_y  # y2\n",
    "            \n",
    "            # Convert to tensors for metrics\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if boxes_tensor.dim() == 1 and len(boxes_tensor) > 0:\n",
    "                boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "            if scores_tensor.dim() == 0 and len(scores_tensor) > 0:\n",
    "                scores_tensor = scores_tensor.unsqueeze(0)\n",
    "            if labels_tensor.dim() == 0 and len(labels_tensor) > 0:\n",
    "                labels_tensor = labels_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Prepare predictions for metrics\n",
    "            pred_metrics = [{\n",
    "                'boxes': boxes_tensor,\n",
    "                'scores': scores_tensor,\n",
    "                'labels': labels_tensor\n",
    "            }]\n",
    "            \n",
    "            # Prepare targets\n",
    "            target_boxes = batch['boxes'][i].cpu().float()\n",
    "            target_labels = batch['labels'][i].cpu().long()\n",
    "            \n",
    "            if target_boxes.dim() == 1 and len(target_boxes) > 0:\n",
    "                target_boxes = target_boxes.unsqueeze(0)\n",
    "            if target_labels.dim() == 0 and len(target_labels) > 0:\n",
    "                target_labels = target_labels.unsqueeze(0)\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': target_boxes,\n",
    "                'labels': target_labels\n",
    "            }]\n",
    "            \n",
    "            # Update metrics\n",
    "            try:\n",
    "                self.metric.update(pred_metrics, targets)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating metrics for image {image_paths[i]}: {e}\")\n",
    "                print(f\"Prediction boxes shape: {boxes_tensor.shape}\")\n",
    "                print(f\"Target boxes shape: {target_boxes.shape}\")\n",
    "                print(f\"Sample prediction boxes: {boxes_tensor[:2]}\")\n",
    "                print(f\"Sample target boxes: {target_boxes[:2]}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image_path': image_paths[i],\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'labels': [self.model.names[int(x)] for x in labels] if len(labels) > 0 else [],\n",
    "                'time': inference_time / batch_size  # Average time per image\n",
    "            })\n",
    "        \n",
    "        return batch_results\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset = COCODataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Please verify the dataset path and files exist\")\n",
    "        raise\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    map_50_values = []\n",
    "    inference_times = []\n",
    "    batch_sizes = BATCH_SIZES\n",
    "\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating with batch size: {batch_size}\")\n",
    "        print(f\"Processing {len(dataset)} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=lambda x: x\n",
    "        )\n",
    "        \n",
    "        evaluator = YOLOv5Evaluator()\n",
    "        total_images = 0\n",
    "        total_time = 0\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_dict = {\n",
    "                'image': torch.stack([item['image'] for item in batch]),\n",
    "                'original_image': [item['original_image'] for item in batch],\n",
    "                'boxes': [item['boxes'] for item in batch],\n",
    "                'labels': [item['labels'] for item in batch],\n",
    "                'original_size': [item['original_size'] for item in batch],\n",
    "                'image_id': [item['image_id'] for item in batch],\n",
    "                'image_path': [item['image_path'] for item in batch]\n",
    "            }\n",
    "            \n",
    "            batch_results = evaluator.evaluate_batch(batch_dict)\n",
    "            results.extend(batch_results)\n",
    "            total_images += len(batch_results)\n",
    "            total_time += sum(r['time'] for r in batch_results)\n",
    "            \n",
    "            print(f\"Processed batch {batch_idx+1} with {len(batch)} images\")\n",
    "        \n",
    "        metrics = evaluator.metric.compute()\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"mAP@0.5: {metrics['map_50'].item():.3f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics['map'].item():.3f}\")\n",
    "        print(f\"Recall@100: {metrics['mar_100'].item():.3f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Images Processed: {total_images}\")\n",
    "        print(f\"Total Inference Time: {total_time:.4f}s\")\n",
    "        print(f\"Average Inference Time: {total_time/total_images:.4f}s per image\")\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        map_50_values.append(metrics['map_50'].item())\n",
    "        inference_times.append(total_time / total_images)\n",
    "        \n",
    "        # Save visualizations\n",
    "        os.makedirs(f\"visualizations_batch_{batch_size}\", exist_ok=True)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(result['image_path'])\n",
    "                fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                for box, label, score in zip(result['boxes'], result['labels'], result['scores']):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(\n",
    "                        x1, y1-10, \n",
    "                        f\"{label} {score:.2f}\",\n",
    "                        color='white', fontsize=10,\n",
    "                        bbox=dict(facecolor='red', alpha=0.8, pad=2)\n",
    "                    )\n",
    "                \n",
    "                plt.axis('off')\n",
    "                save_path = os.path.join(f\"visualizations_batch_{batch_size}\", f\"result_{idx}.png\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not visualize {result['image_path']}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Saved visualizations to visualizations_batch_{batch_size} folder\")\n",
    "    \n",
    "    # Plot mAP vs Inference Time\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP on left y-axis\n",
    "    ax1.plot(batch_sizes, map_50_values, 'b-', marker='o', label='mAP@0.5')\n",
    "    ax1.set_xlabel('Batch Size')\n",
    "    ax1.set_ylabel('mAP@0.5', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Create right y-axis for inference time\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(batch_sizes, inference_times, 'r-', marker='s', label='Inference Time (s)')\n",
    "    ax2.set_ylabel('Inference Time (s)', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Title and layout\n",
    "    plt.title('mAP@0.5 vs Inference Time for Different Batch Sizes')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "    # Save and show plot\n",
    "    plt.savefig('map_vs_inference_time.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d966be4-372a-42b1-8a1b-5705207a9c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.8-cp39-cp39-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from pycocotools) (3.9.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from pycocotools) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siu856526097\\appdata\\local\\anaconda3\\envs\\torch_quant1\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Using cached pycocotools-2.0.8-cp39-cp39-win_amd64.whl (85 kB)\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41be36b0-cc6b-484b-833d-94f5dc72df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotation file: C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\\annotations\\instances_val2017.json\n",
      "Selected 32 images for evaluation\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 1\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 1 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 3 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 4 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 5 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 6 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 7 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 8 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 9 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 10 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 11 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 12 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 13 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 14 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 15 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 16 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 17 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 18 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 19 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 20 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Processed batch 21 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 22 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 23 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 24 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 25 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 26 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 27 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 28 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 29 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 30 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 31 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 32 with 1 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 39.7869s\n",
      "Average Inference Time: 1.2433s per image\n",
      "Saved visualizations to visualizations_batch_1 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 4\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 1 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 2 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Processed batch 3 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 4 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 5 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 6 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 7 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 8 with 4 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 35.9135s\n",
      "Average Inference Time: 1.1223s per image\n",
      "Saved visualizations to visualizations_batch_4 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 8\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 79 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 56 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 108 detections\n",
      "After NMS: 9 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 73 detections\n",
      "After NMS: 6 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 93 detections\n",
      "After NMS: 9 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 3 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 4 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 26 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 1 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 125 detections\n",
      "After NMS: 12 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 181 detections\n",
      "After NMS: 13 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 2 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 157 detections\n",
      "After NMS: 13 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 129 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 99 detections\n",
      "After NMS: 8 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 32 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 3 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 15 detections\n",
      "After NMS: 2 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 105 detections\n",
      "After NMS: 14 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 86 detections\n",
      "After NMS: 8 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 97 detections\n",
      "After NMS: 9 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 3 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 36 detections\n",
      "After NMS: 5 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 38 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 67 detections\n",
      "After NMS: 6 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 80 detections\n",
      "After NMS: 7 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 113 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 7 detections\n",
      "After NMS: 1 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 3 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 9 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 4 with 8 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.654\n",
      "mAP@0.5-0.95: 0.654\n",
      "Recall@100: 0.553\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 34.7787s\n",
      "Average Inference Time: 1.0868s per image\n",
      "Saved visualizations to visualizations_batch_8 folder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from typing import List, Dict\n",
    "from torchvision.ops import nms\n",
    "# Configuration\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IMG_SIZE = 640\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_PATH = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\"\n",
    "MAX_IMAGES = 32  # Only process first 32 images\n",
    "BATCH_SIZES = [1, 4, 8]  # Batch sizes to evaluate\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_path, split='val2017', max_images=MAX_IMAGES):\n",
    "        \"\"\"Initialize COCO dataset with limited images\"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.image_dir = os.path.join(root_path, 'images', split)\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        # Check for annotation files\n",
    "        self.annotation_file = os.path.join(root_path, 'annotations', f'person_keypoints_{split}.json')\n",
    "        if not os.path.exists(self.annotation_file):\n",
    "            self.annotation_file = os.path.join(root_path, 'annotations', f'instances_{split}.json')\n",
    "            if not os.path.exists(self.annotation_file):\n",
    "                raise FileNotFoundError(f\"Neither person_keypoints_{split}.json nor instances_{split}.json found\")\n",
    "        \n",
    "        print(f\"Using annotation file: {self.annotation_file}\")\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Create image id to annotations mapping\n",
    "        self.image_info = {img['id']: img for img in self.annotations['images']}\n",
    "        self.annotations_per_image = {}\n",
    "        \n",
    "        for ann in self.annotations['annotations']:\n",
    "            if ann['image_id'] not in self.annotations_per_image:\n",
    "                self.annotations_per_image[ann['image_id']] = []\n",
    "            self.annotations_per_image[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Filter images with annotations and limit to max_images\n",
    "        self.valid_image_ids = [img_id for img_id in self.image_info.keys() \n",
    "                              if img_id in self.annotations_per_image][:max_images]\n",
    "        \n",
    "        print(f\"Selected {len(self.valid_image_ids)} images for evaluation\")\n",
    "        \n",
    "        # Transform for input images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_image_ids[idx]\n",
    "        img_info = self.image_info[img_id]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            return {\n",
    "                'image': torch.zeros((3, IMG_SIZE, IMG_SIZE)),\n",
    "                'original_image': Image.new('RGB', (IMG_SIZE, IMG_SIZE)),\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'original_size': (IMG_SIZE, IMG_SIZE),\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        \n",
    "        original_size = img.size  # (width, height)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annotations = self.annotations_per_image[img_id]\n",
    "        \n",
    "        # Prepare ground truth boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(0)  # 0 is for person class in COCO\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'original_image': img,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': original_size,\n",
    "            'image_id': img_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "from torchvision.ops import nms\n",
    "\n",
    "class YOLOv5Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize YOLOv5 model with evaluation capabilities\"\"\"\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True, autoshape=True, force_reload=True)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metric = MeanAveragePrecision(\n",
    "            box_format='xyxy',\n",
    "            iou_type='bbox',\n",
    "            iou_thresholds=[0.5],\n",
    "            rec_thresholds=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True\n",
    "        )\n",
    "\n",
    "    def evaluate_batch(self, batch: Dict) -> List[Dict]:\n",
    "        \"\"\"Evaluate batch of images with raw tensor handling\"\"\"\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        original_sizes = batch['original_size']\n",
    "        image_paths = batch['image_path']\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = self.model(images, size=IMG_SIZE)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Debug\n",
    "        print(f\"Results type: {type(results)}\")\n",
    "        if isinstance(results, torch.Tensor):\n",
    "            print(f\"Raw results shape: {results.shape}\")\n",
    "        else:\n",
    "            print(f\"Results attributes: {dir(results)}\")\n",
    "        \n",
    "        # Ensure results is a tensor\n",
    "        if not isinstance(results, torch.Tensor):\n",
    "            raise ValueError(f\"Expected torch.Tensor, got {type(results)}\")\n",
    "        if results.dim() != 3 or results.shape[0] != batch_size:\n",
    "            raise ValueError(f\"Unexpected results shape: {results.shape}. Expected [batch_size, num_detections, 85]\")\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(batch_size):\n",
    "            pred = results[i]  # [num_detections, 85]\n",
    "            print(f\"Image {i} predictions shape: {pred.shape}\")\n",
    "            \n",
    "            # Extract components\n",
    "            if pred.shape[0] == 0:\n",
    "                boxes = np.zeros((0, 4))\n",
    "                scores = np.zeros(0)\n",
    "                labels = np.zeros(0, dtype=np.int64)\n",
    "            else:\n",
    "                # Convert to xyxy format\n",
    "                x_center = pred[:, 0]\n",
    "                y_center = pred[:, 1]\n",
    "                w = pred[:, 2]\n",
    "                h = pred[:, 3]\n",
    "                conf = pred[:, 4]\n",
    "                class_scores = pred[:, 5:]\n",
    "                \n",
    "                x1 = x_center - w / 2\n",
    "                y1 = y_center - h / 2\n",
    "                x2 = x_center + w / 2\n",
    "                y2 = y_center + h / 2\n",
    "                boxes = torch.stack([x1, y1, x2, y2], dim=1)  # [N, 4]\n",
    "                scores = conf  # [N]\n",
    "                labels = torch.argmax(class_scores, dim=1)  # [N]\n",
    "                \n",
    "                # Apply confidence filter first\n",
    "                mask = scores >= CONFIDENCE_THRESHOLD\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                print(f\"After confidence filter: {boxes.shape[0]} detections\")\n",
    "                \n",
    "                # Apply NMS\n",
    "                if boxes.shape[0] > 0:\n",
    "                    keep = nms(boxes, scores, iou_threshold=0.45)  # IoU threshold for NMS\n",
    "                    boxes = boxes[keep].cpu().numpy()\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = labels[keep].cpu().numpy()\n",
    "                    print(f\"After NMS: {boxes.shape[0]} detections\")\n",
    "                else:\n",
    "                    boxes = np.zeros((0, 4))\n",
    "                    scores = np.zeros(0)\n",
    "                    labels = np.zeros(0, dtype=np.int64)\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            scale_x = orig_w / IMG_SIZE\n",
    "            scale_y = orig_h / IMG_SIZE\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, 0] *= scale_x  # x1\n",
    "                boxes[:, 1] *= scale_y  # y1\n",
    "                boxes[:, 2] *= scale_x  # x2\n",
    "                boxes[:, 3] *= scale_y  # y2\n",
    "            \n",
    "            # Convert to tensors for metrics\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if boxes_tensor.dim() == 1 and len(boxes_tensor) > 0:\n",
    "                boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "            if scores_tensor.dim() == 0 and len(scores_tensor) > 0:\n",
    "                scores_tensor = scores_tensor.unsqueeze(0)\n",
    "            if labels_tensor.dim() == 0 and len(labels_tensor) > 0:\n",
    "                labels_tensor = labels_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Prepare predictions for metrics\n",
    "            pred_metrics = [{\n",
    "                'boxes': boxes_tensor,\n",
    "                'scores': scores_tensor,\n",
    "                'labels': labels_tensor\n",
    "            }]\n",
    "            \n",
    "            # Prepare targets\n",
    "            target_boxes = batch['boxes'][i].cpu().float()\n",
    "            target_labels = batch['labels'][i].cpu().long()\n",
    "            \n",
    "            if target_boxes.dim() == 1 and len(target_boxes) > 0:\n",
    "                target_boxes = target_boxes.unsqueeze(0)\n",
    "            if target_labels.dim() == 0 and len(target_labels) > 0:\n",
    "                target_labels = target_labels.unsqueeze(0)\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': target_boxes,\n",
    "                'labels': target_labels\n",
    "            }]\n",
    "            \n",
    "            # Update metrics\n",
    "            try:\n",
    "                self.metric.update(pred_metrics, targets)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating metrics for image {image_paths[i]}: {e}\")\n",
    "                print(f\"Prediction boxes shape: {boxes_tensor.shape}\")\n",
    "                print(f\"Target boxes shape: {target_boxes.shape}\")\n",
    "                print(f\"Sample prediction boxes: {boxes_tensor[:2]}\")\n",
    "                print(f\"Sample target boxes: {target_boxes[:2]}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image_path': image_paths[i],\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'labels': [self.model.names[int(x)] for x in labels] if len(labels) > 0 else [],\n",
    "                'time': inference_time / batch_size  # Average time per image\n",
    "            })\n",
    "        \n",
    "        return batch_results\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset = COCODataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Please verify the dataset path and files exist\")\n",
    "        raise\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    map_50_values = []\n",
    "    map_values = []\n",
    "    mar_100_values = []\n",
    "    inference_times_per_image = []\n",
    "    total_inference_times = []  # For total inference time\n",
    "    batch_sizes = BATCH_SIZES\n",
    "\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating with batch size: {batch_size}\")\n",
    "        print(f\"Processing {len(dataset)} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=lambda x: x\n",
    "        )\n",
    "        \n",
    "        evaluator = YOLOv5Evaluator()\n",
    "        total_images = 0\n",
    "        total_time = 0\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_dict = {\n",
    "                'image': torch.stack([item['image'] for item in batch]),\n",
    "                'original_image': [item['original_image'] for item in batch],\n",
    "                'boxes': [item['boxes'] for item in batch],\n",
    "                'labels': [item['labels'] for item in batch],\n",
    "                'original_size': [item['original_size'] for item in batch],\n",
    "                'image_id': [item['image_id'] for item in batch],\n",
    "                'image_path': [item['image_path'] for item in batch]\n",
    "            }\n",
    "            \n",
    "            batch_results = evaluator.evaluate_batch(batch_dict)\n",
    "            results.extend(batch_results)\n",
    "            total_images += len(batch_results)\n",
    "            total_time += sum(r['time'] for r in batch_results)\n",
    "            \n",
    "            print(f\"Processed batch {batch_idx+1} with {len(batch)} images\")\n",
    "        \n",
    "        metrics = evaluator.metric.compute()\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"mAP@0.5: {metrics['map_50'].item():.3f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics['map'].item():.3f}\")\n",
    "        print(f\"Recall@100: {metrics['mar_100'].item():.3f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Images Processed: {total_images}\")\n",
    "        print(f\"Total Inference Time: {total_time:.4f}s\")\n",
    "        print(f\"Average Inference Time: {total_time/total_images:.4f}s per image\")\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        map_50_values.append(metrics['map_50'].item())\n",
    "        map_values.append(metrics['map'].item())\n",
    "        mar_100_values.append(metrics['mar_100'].item())\n",
    "        inference_times_per_image.append(total_time / total_images)\n",
    "        total_inference_times.append(total_time)\n",
    "        \n",
    "        # Save visualizations\n",
    "        os.makedirs(f\"visualizations_batch_{batch_size}\", exist_ok=True)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(result['image_path'])\n",
    "                fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                for box, label, score in zip(result['boxes'], result['labels'], result['scores']):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(\n",
    "                        x1, y1-10, \n",
    "                        f\"{label} {score:.2f}\",\n",
    "                        color='white', fontsize=10,\n",
    "                        bbox=dict(facecolor='red', alpha=0.8, pad=2)\n",
    "                    )\n",
    "                \n",
    "                plt.axis('off')\n",
    "                save_path = os.path.join(f\"visualizations_batch_{batch_size}\", f\"result_{idx}.png\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not visualize {result['image_path']}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Saved visualizations to visualizations_batch_{batch_size} folder\")\n",
    "    \n",
    "    # Plot multiple graphs\n",
    "    def plot_dual_axis(metric_values, metric_name, time_values, time_label, filename):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Plot metric on left y-axis\n",
    "        ax1.plot(batch_sizes, metric_values, 'b-', marker='o', label=metric_name)\n",
    "        ax1.set_xlabel('Batch Size')\n",
    "        ax1.set_ylabel(metric_name, color='b')\n",
    "        ax1.tick_params(axis='y', labelcolor='b')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Create right y-axis for time\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(batch_sizes, time_values, 'r-', marker='s', label=time_label)\n",
    "        ax2.set_ylabel(time_label, color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "        # Title and layout\n",
    "        plt.title(f'{metric_name} vs {time_label} for Different Batch Sizes')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Combine legends\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "        # Save and show plot\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    # Generate plots\n",
    "    # Plot 1: mAP@0.5 vs Inference Time (per image)\n",
    "    plot_dual_axis(\n",
    "        metric_values=map_50_values,\n",
    "        metric_name='mAP@0.5',\n",
    "        time_values=inference_times_per_image,\n",
    "        time_label='Inference Time (s/image)',\n",
    "        filename='map50_vs_inference_time.png'\n",
    "    )\n",
    "    \n",
    "    # Plot 2: mAP@0.5:0.95 vs Total Inference Time\n",
    "    plot_dual_axis(\n",
    "        metric_values=map_values,\n",
    "        metric_name='mAP@0.5:0.95',\n",
    "        time_values=total_inference_times,\n",
    "        time_label='Total Inference Time (s)',\n",
    "        filename='map_vs_total_inference_time.png'\n",
    "    )\n",
    "    \n",
    "    # Plot 3: Recall@100 vs Inference Time (per image)\n",
    "    plot_dual_axis(\n",
    "        metric_values=mar_100_values,\n",
    "        metric_name='Recall@100',\n",
    "        time_values=inference_times_per_image,\n",
    "        time_label='Inference Time (s/image)',\n",
    "        filename='mar100_vs_inference_time.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db285751-b5bd-4b97-bb38-c2b3604f2c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using annotation file: C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\\annotations\\instances_val2017.json\n",
      "Selected 32 images for evaluation\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 1\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 45 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 1 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 75 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 3 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 50 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 4 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 81 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 5 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 25 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 6 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 13 detections\n",
      "After NMS: 1 detections\n",
      "Processed batch 7 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 20 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 8 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 65 detections\n",
      "After NMS: 8 detections\n",
      "Processed batch 9 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 143 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 10 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 0 detections\n",
      "Processed batch 11 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 135 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 12 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 95 detections\n",
      "After NMS: 10 detections\n",
      "Processed batch 13 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 14 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 72 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 15 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 21 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 16 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 17 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 17 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 18 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 22 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 19 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 4 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 20 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 77 detections\n",
      "After NMS: 9 detections\n",
      "Processed batch 21 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 57 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 22 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 91 detections\n",
      "After NMS: 11 detections\n",
      "Processed batch 23 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 24 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 25 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 43 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 26 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 27 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 69 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 28 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 48 detections\n",
      "After NMS: 7 detections\n",
      "Processed batch 29 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 10 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 30 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 31 with 1 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([1, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 8 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 32 with 1 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.639\n",
      "mAP@0.5-0.95: 0.639\n",
      "Recall@100: 0.503\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 17.8736s\n",
      "Average Inference Time: 0.5586s per image\n",
      "Saved visualizations to visualizations_batch_1 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 4\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 45 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 75 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 50 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 1 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 81 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 25 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 13 detections\n",
      "After NMS: 1 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 20 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 2 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 65 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 143 detections\n",
      "After NMS: 12 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 0 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 135 detections\n",
      "After NMS: 12 detections\n",
      "Processed batch 3 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 95 detections\n",
      "After NMS: 10 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 72 detections\n",
      "After NMS: 7 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 21 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 4 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 17 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 22 detections\n",
      "After NMS: 2 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 4 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 5 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 77 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 57 detections\n",
      "After NMS: 6 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 91 detections\n",
      "After NMS: 11 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 6 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 43 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 7 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 69 detections\n",
      "After NMS: 6 detections\n",
      "Processed batch 7 with 4 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([4, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 48 detections\n",
      "After NMS: 7 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 10 detections\n",
      "After NMS: 2 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 5 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 8 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 8 with 4 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.639\n",
      "mAP@0.5-0.95: 0.639\n",
      "Recall@100: 0.503\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 17.2531s\n",
      "Average Inference Time: 0.5392s per image\n",
      "Saved visualizations to visualizations_batch_4 folder\n",
      "\n",
      "==================================================\n",
      "Evaluating with batch size: 8\n",
      "Processing 32 images\n",
      "==================================================\n",
      "PyTorch version: 1.13.1+cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SIU856526097/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2025-4-13 Python-3.9.21 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 45 detections\n",
      "After NMS: 9 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 34 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 75 detections\n",
      "After NMS: 8 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 50 detections\n",
      "After NMS: 5 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 81 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 25 detections\n",
      "After NMS: 4 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 13 detections\n",
      "After NMS: 1 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 20 detections\n",
      "After NMS: 4 detections\n",
      "Processed batch 1 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 65 detections\n",
      "After NMS: 8 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 143 detections\n",
      "After NMS: 12 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 0 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 135 detections\n",
      "After NMS: 12 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 95 detections\n",
      "After NMS: 10 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 5 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 72 detections\n",
      "After NMS: 7 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 21 detections\n",
      "After NMS: 3 detections\n",
      "Processed batch 2 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 61 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 17 detections\n",
      "After NMS: 3 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 22 detections\n",
      "After NMS: 2 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 4 detections\n",
      "After NMS: 2 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 77 detections\n",
      "After NMS: 9 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 57 detections\n",
      "After NMS: 6 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 91 detections\n",
      "After NMS: 11 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 30 detections\n",
      "After NMS: 5 detections\n",
      "Processed batch 3 with 8 images\n",
      "Results type: <class 'torch.Tensor'>\n",
      "Raw results shape: torch.Size([8, 25200, 85])\n",
      "Image 0 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 4 detections\n",
      "Image 1 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 43 detections\n",
      "After NMS: 4 detections\n",
      "Image 2 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 74 detections\n",
      "After NMS: 7 detections\n",
      "Image 3 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 69 detections\n",
      "After NMS: 6 detections\n",
      "Image 4 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 48 detections\n",
      "After NMS: 7 detections\n",
      "Image 5 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 10 detections\n",
      "After NMS: 2 detections\n",
      "Image 6 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 24 detections\n",
      "After NMS: 5 detections\n",
      "Image 7 predictions shape: torch.Size([25200, 85])\n",
      "After confidence filter: 8 detections\n",
      "After NMS: 2 detections\n",
      "Processed batch 4 with 8 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP@0.5: 0.639\n",
      "mAP@0.5-0.95: 0.639\n",
      "Recall@100: 0.503\n",
      "\n",
      "Total Images Processed: 32\n",
      "Total Inference Time: 16.0098s\n",
      "Average Inference Time: 0.5003s per image\n",
      "Saved visualizations to visualizations_batch_8 folder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from typing import List, Dict\n",
    "from torchvision.ops import nms\n",
    "# Configuration\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IMG_SIZE = 640\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_PATH = r\"C:\\Users\\SIU856526097\\datasets\\coco-pose-subset\"\n",
    "MAX_IMAGES = 32  # Only process first 32 images\n",
    "BATCH_SIZES = [1, 4, 8]  # Batch sizes to evaluate\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_path, split='val2017', max_images=MAX_IMAGES):\n",
    "        \"\"\"Initialize COCO dataset with limited images\"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.image_dir = os.path.join(root_path, 'images', split)\n",
    "        self.max_images = max_images\n",
    "        \n",
    "        # Check for annotation files\n",
    "        self.annotation_file = os.path.join(root_path, 'annotations', f'person_keypoints_{split}.json')\n",
    "        if not os.path.exists(self.annotation_file):\n",
    "            self.annotation_file = os.path.join(root_path, 'annotations', f'instances_{split}.json')\n",
    "            if not os.path.exists(self.annotation_file):\n",
    "                raise FileNotFoundError(f\"Neither person_keypoints_{split}.json nor instances_{split}.json found\")\n",
    "        \n",
    "        print(f\"Using annotation file: {self.annotation_file}\")\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Create image id to annotations mapping\n",
    "        self.image_info = {img['id']: img for img in self.annotations['images']}\n",
    "        self.annotations_per_image = {}\n",
    "        \n",
    "        for ann in self.annotations['annotations']:\n",
    "            if ann['image_id'] not in self.annotations_per_image:\n",
    "                self.annotations_per_image[ann['image_id']] = []\n",
    "            self.annotations_per_image[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Filter images with annotations and limit to max_images\n",
    "        self.valid_image_ids = [img_id for img_id in self.image_info.keys() \n",
    "                              if img_id in self.annotations_per_image][:max_images]\n",
    "        \n",
    "        print(f\"Selected {len(self.valid_image_ids)} images for evaluation\")\n",
    "        \n",
    "        # Transform for input images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_image_ids[idx]\n",
    "        img_info = self.image_info[img_id]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            return {\n",
    "                'image': torch.zeros((3, IMG_SIZE, IMG_SIZE)),\n",
    "                'original_image': Image.new('RGB', (IMG_SIZE, IMG_SIZE)),\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'original_size': (IMG_SIZE, IMG_SIZE),\n",
    "                'image_id': img_id,\n",
    "                'image_path': img_path\n",
    "            }\n",
    "        \n",
    "        original_size = img.size  # (width, height)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annotations = self.annotations_per_image[img_id]\n",
    "        \n",
    "        # Prepare ground truth boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(0)  # 0 is for person class in COCO\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        # Apply transformations\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'original_image': img,\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'original_size': original_size,\n",
    "            'image_id': img_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "from torchvision.ops import nms\n",
    "\n",
    "class YOLOv5Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize YOLOv5 model with evaluation capabilities\"\"\"\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, autoshape=True, force_reload=True)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.metric = MeanAveragePrecision(\n",
    "            box_format='xyxy',\n",
    "            iou_type='bbox',\n",
    "            iou_thresholds=[0.5],\n",
    "            rec_thresholds=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            max_detection_thresholds=[1, 10, 100],\n",
    "            class_metrics=True\n",
    "        )\n",
    "\n",
    "    def evaluate_batch(self, batch: Dict) -> List[Dict]:\n",
    "        \"\"\"Evaluate batch of images with raw tensor handling\"\"\"\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        original_sizes = batch['original_size']\n",
    "        image_paths = batch['image_path']\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            results = self.model(images, size=IMG_SIZE)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Debug\n",
    "        print(f\"Results type: {type(results)}\")\n",
    "        if isinstance(results, torch.Tensor):\n",
    "            print(f\"Raw results shape: {results.shape}\")\n",
    "        else:\n",
    "            print(f\"Results attributes: {dir(results)}\")\n",
    "        \n",
    "        # Ensure results is a tensor\n",
    "        if not isinstance(results, torch.Tensor):\n",
    "            raise ValueError(f\"Expected torch.Tensor, got {type(results)}\")\n",
    "        if results.dim() != 3 or results.shape[0] != batch_size:\n",
    "            raise ValueError(f\"Unexpected results shape: {results.shape}. Expected [batch_size, num_detections, 85]\")\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(batch_size):\n",
    "            pred = results[i]  # [num_detections, 85]\n",
    "            print(f\"Image {i} predictions shape: {pred.shape}\")\n",
    "            \n",
    "            # Extract components\n",
    "            if pred.shape[0] == 0:\n",
    "                boxes = np.zeros((0, 4))\n",
    "                scores = np.zeros(0)\n",
    "                labels = np.zeros(0, dtype=np.int64)\n",
    "            else:\n",
    "                # Convert to xyxy format\n",
    "                x_center = pred[:, 0]\n",
    "                y_center = pred[:, 1]\n",
    "                w = pred[:, 2]\n",
    "                h = pred[:, 3]\n",
    "                conf = pred[:, 4]\n",
    "                class_scores = pred[:, 5:]\n",
    "                \n",
    "                x1 = x_center - w / 2\n",
    "                y1 = y_center - h / 2\n",
    "                x2 = x_center + w / 2\n",
    "                y2 = y_center + h / 2\n",
    "                boxes = torch.stack([x1, y1, x2, y2], dim=1)  # [N, 4]\n",
    "                scores = conf  # [N]\n",
    "                labels = torch.argmax(class_scores, dim=1)  # [N]\n",
    "                \n",
    "                # Apply confidence filter first\n",
    "                mask = scores >= CONFIDENCE_THRESHOLD\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                print(f\"After confidence filter: {boxes.shape[0]} detections\")\n",
    "                \n",
    "                # Apply NMS\n",
    "                if boxes.shape[0] > 0:\n",
    "                    keep = nms(boxes, scores, iou_threshold=0.45)  # IoU threshold for NMS\n",
    "                    boxes = boxes[keep].cpu().numpy()\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = labels[keep].cpu().numpy()\n",
    "                    print(f\"After NMS: {boxes.shape[0]} detections\")\n",
    "                else:\n",
    "                    boxes = np.zeros((0, 4))\n",
    "                    scores = np.zeros(0)\n",
    "                    labels = np.zeros(0, dtype=np.int64)\n",
    "            \n",
    "            # Scale boxes back to original image size\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            scale_x = orig_w / IMG_SIZE\n",
    "            scale_y = orig_h / IMG_SIZE\n",
    "            \n",
    "            if len(boxes) > 0:\n",
    "                boxes[:, 0] *= scale_x  # x1\n",
    "                boxes[:, 1] *= scale_y  # y1\n",
    "                boxes[:, 2] *= scale_x  # x2\n",
    "                boxes[:, 3] *= scale_y  # y2\n",
    "            \n",
    "            # Convert to tensors for metrics\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if boxes_tensor.dim() == 1 and len(boxes_tensor) > 0:\n",
    "                boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "            if scores_tensor.dim() == 0 and len(scores_tensor) > 0:\n",
    "                scores_tensor = scores_tensor.unsqueeze(0)\n",
    "            if labels_tensor.dim() == 0 and len(labels_tensor) > 0:\n",
    "                labels_tensor = labels_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Prepare predictions for metrics\n",
    "            pred_metrics = [{\n",
    "                'boxes': boxes_tensor,\n",
    "                'scores': scores_tensor,\n",
    "                'labels': labels_tensor\n",
    "            }]\n",
    "            \n",
    "            # Prepare targets\n",
    "            target_boxes = batch['boxes'][i].cpu().float()\n",
    "            target_labels = batch['labels'][i].cpu().long()\n",
    "            \n",
    "            if target_boxes.dim() == 1 and len(target_boxes) > 0:\n",
    "                target_boxes = target_boxes.unsqueeze(0)\n",
    "            if target_labels.dim() == 0 and len(target_labels) > 0:\n",
    "                target_labels = target_labels.unsqueeze(0)\n",
    "            \n",
    "            targets = [{\n",
    "                'boxes': target_boxes,\n",
    "                'labels': target_labels\n",
    "            }]\n",
    "            \n",
    "            # Update metrics\n",
    "            try:\n",
    "                self.metric.update(pred_metrics, targets)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating metrics for image {image_paths[i]}: {e}\")\n",
    "                print(f\"Prediction boxes shape: {boxes_tensor.shape}\")\n",
    "                print(f\"Target boxes shape: {target_boxes.shape}\")\n",
    "                print(f\"Sample prediction boxes: {boxes_tensor[:2]}\")\n",
    "                print(f\"Sample target boxes: {target_boxes[:2]}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image_path': image_paths[i],\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'labels': [self.model.names[int(x)] for x in labels] if len(labels) > 0 else [],\n",
    "                'time': inference_time / batch_size  # Average time per image\n",
    "            })\n",
    "        \n",
    "        return batch_results\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset = COCODataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Please verify the dataset path and files exist\")\n",
    "        raise\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    map_50_values = []\n",
    "    map_values = []\n",
    "    mar_100_values = []\n",
    "    inference_times_per_image = []\n",
    "    total_inference_times = []  # For total inference time\n",
    "    batch_sizes = BATCH_SIZES\n",
    "\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating with batch size: {batch_size}\")\n",
    "        print(f\"Processing {len(dataset)} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=lambda x: x\n",
    "        )\n",
    "        \n",
    "        evaluator = YOLOv5Evaluator()\n",
    "        total_images = 0\n",
    "        total_time = 0\n",
    "        results = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch_dict = {\n",
    "                'image': torch.stack([item['image'] for item in batch]),\n",
    "                'original_image': [item['original_image'] for item in batch],\n",
    "                'boxes': [item['boxes'] for item in batch],\n",
    "                'labels': [item['labels'] for item in batch],\n",
    "                'original_size': [item['original_size'] for item in batch],\n",
    "                'image_id': [item['image_id'] for item in batch],\n",
    "                'image_path': [item['image_path'] for item in batch]\n",
    "            }\n",
    "            \n",
    "            batch_results = evaluator.evaluate_batch(batch_dict)\n",
    "            results.extend(batch_results)\n",
    "            total_images += len(batch_results)\n",
    "            total_time += sum(r['time'] for r in batch_results)\n",
    "            \n",
    "            print(f\"Processed batch {batch_idx+1} with {len(batch)} images\")\n",
    "        \n",
    "        metrics = evaluator.metric.compute()\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"mAP@0.5: {metrics['map_50'].item():.3f}\")\n",
    "        print(f\"mAP@0.5-0.95: {metrics['map'].item():.3f}\")\n",
    "        print(f\"Recall@100: {metrics['mar_100'].item():.3f}\")\n",
    "        \n",
    "        print(f\"\\nTotal Images Processed: {total_images}\")\n",
    "        print(f\"Total Inference Time: {total_time:.4f}s\")\n",
    "        print(f\"Average Inference Time: {total_time/total_images:.4f}s per image\")\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        map_50_values.append(metrics['map_50'].item())\n",
    "        map_values.append(metrics['map'].item())\n",
    "        mar_100_values.append(metrics['mar_100'].item())\n",
    "        inference_times_per_image.append(total_time / total_images)\n",
    "        total_inference_times.append(total_time)\n",
    "        \n",
    "        # Save visualizations\n",
    "        os.makedirs(f\"visualizations_batch_{batch_size}\", exist_ok=True)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            try:\n",
    "                img = Image.open(result['image_path'])\n",
    "                fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                for box, label, score in zip(result['boxes'], result['labels'], result['scores']):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(\n",
    "                        x1, y1-10, \n",
    "                        f\"{label} {score:.2f}\",\n",
    "                        color='white', fontsize=10,\n",
    "                        bbox=dict(facecolor='red', alpha=0.8, pad=2)\n",
    "                    )\n",
    "                \n",
    "                plt.axis('off')\n",
    "                save_path = os.path.join(f\"visualizations_batch_{batch_size}\", f\"result_{idx}.png\")\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not visualize {result['image_path']}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Saved visualizations to visualizations_batch_{batch_size} folder\")\n",
    "    \n",
    "    # Plot multiple graphs\n",
    "    def plot_dual_axis(metric_values, metric_name, time_values, time_label, filename):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Plot metric on left y-axis\n",
    "        ax1.plot(batch_sizes, metric_values, 'b-', marker='o', label=metric_name)\n",
    "        ax1.set_xlabel('Batch Size')\n",
    "        ax1.set_ylabel(metric_name, color='b')\n",
    "        ax1.tick_params(axis='y', labelcolor='b')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Create right y-axis for time\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(batch_sizes, time_values, 'r-', marker='s', label=time_label)\n",
    "        ax2.set_ylabel(time_label, color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "        # Title and layout\n",
    "        plt.title(f'{metric_name} vs {time_label} for Different Batch Sizes')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Combine legends\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "        # Save and show plot\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    # Generate plots\n",
    "    # Plot 1: mAP@0.5 vs Inference Time (per image)\n",
    "    plot_dual_axis(\n",
    "        metric_values=map_50_values,\n",
    "        metric_name='mAP@0.5',\n",
    "        time_values=inference_times_per_image,\n",
    "        time_label='Inference Time (s/image)',\n",
    "        filename='map50_vs_inference_time.png'\n",
    "    )\n",
    "    \n",
    "    # Plot 2: mAP@0.5:0.95 vs Total Inference Time\n",
    "    plot_dual_axis(\n",
    "        metric_values=map_values,\n",
    "        metric_name='mAP@0.5:0.95',\n",
    "        time_values=total_inference_times,\n",
    "        time_label='Total Inference Time (s)',\n",
    "        filename='map_vs_total_inference_time.png'\n",
    "    )\n",
    "    \n",
    "    # Plot 3: Recall@100 vs Inference Time (per image)\n",
    "    plot_dual_axis(\n",
    "        metric_values=mar_100_values,\n",
    "        metric_name='Recall@100',\n",
    "        time_values=inference_times_per_image,\n",
    "        time_label='Inference Time (s/image)',\n",
    "        filename='mar100_vs_inference_time.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4231bfd-225f-4e24-bea3-3057a41579d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
