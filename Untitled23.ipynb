{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6d787-c65b-460d-baec-dfa0cd333388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/1000 | Current accuracy: 82.0%\n",
      "Processed 100/1000 | Current accuracy: 75.0%\n",
      "Processed 150/1000 | Current accuracy: 74.0%\n",
      "Processed 200/1000 | Current accuracy: 74.0%\n",
      "Processed 250/1000 | Current accuracy: 74.8%\n",
      "Processed 300/1000 | Current accuracy: 75.0%\n",
      "Processed 350/1000 | Current accuracy: 75.4%\n",
      "Processed 400/1000 | Current accuracy: 76.2%\n",
      "Processed 450/1000 | Current accuracy: 75.1%\n",
      "Processed 500/1000 | Current accuracy: 74.2%\n",
      "\n",
      "Final Results:\n",
      "Accuracy: 74.20%\n",
      "Time: 83.22 seconds\n",
      "Speed: 6.0 images/second\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# 1. Disable problematic features for stability\n",
    "torch.backends.quantized.engine = 'none'  #\n",
    "torch.set_num_threads(1)  \n",
    "# 2. Load standard ResNet18 (most reliable)\n",
    "#model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "#model=models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "#model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "#model=models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "#model=models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "#model=models.vgg11(weights=models.VGG11_Weights.IMAGENET1K_V1)\n",
    "#model=models.vgg11_bn(weights=models.VGG11_BN_Weights.IMAGENET1K_V1)\n",
    "#model=models.vgg13(weights=models.VGG13_Weights.IMAGENET1K_V1)\n",
    "#model=models.vgg13_bn(weights=models.VGG13_BN_Weights.IMAGENET1K_V1)\n",
    "#model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "#model=models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "#model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "#model=models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1)\n",
    "#model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "#model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "#model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "#model=models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "#model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "#model = models.densenet161(weights=models.DenseNet161_Weights.IMAGENET1K_V1)\n",
    "#model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "#model = models.shufflenet_v2_x0_5(weights=models.ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1)\n",
    "#model = models.shufflenet_v2_x2_0(weights=models.ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "#model = models.vit_l_32(weights=models.ViT_L_32_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "#model = models.swin_t(weights=models.Swin_T_Weights.IMAGENET1K_V1)\n",
    "#model = models.swin_b(weights=models.Swin_B_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "#model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "#model = models.convnext_large(weights=models.ConvNeXt_Large_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "#quantized_model.eval()\n",
    "# 3. Minimal transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 4. Single-image processor\n",
    "def process_one_image(folder_path):\n",
    "    try:\n",
    "        img_file = next(f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.jpg','.jpeg','.png')))\n",
    "        img = Image.open(os.path.join(folder_path, img_file)).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# 5. Evaluation\n",
    "#val_root = 'C:/Users/SIU856526097/datasets/val_images/val'\n",
    "val_root = 'C:/Users/SIU856526097/datasets/val_images/val_subset'\n",
    "class_folders = sorted(os.listdir(val_root))[:500]  # All 1000 classes\n",
    "\n",
    "correct = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for class_idx, folder in enumerate(class_folders):\n",
    "        folder_path = os.path.join(val_root, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "            \n",
    "        input_tensor = process_one_image(folder_path)\n",
    "        if input_tensor is None:\n",
    "            continue\n",
    "            \n",
    "        output = model(input_tensor)\n",
    "        predicted_class = torch.argmax(output).item()\n",
    "        correct += (predicted_class == class_idx)\n",
    "\n",
    "        # Progress feedback every 100 images\n",
    "        #if (class_idx+1) % 50 == 0:\n",
    "        #    print(f\"Processed {class_idx+1}/1000 | Current accuracy: {100*correct/(class_idx+1):.1f}%\")\n",
    "         # Progress feedback every 50 images (not 100)\n",
    "        if (class_idx+1) % 50 == 0:\n",
    "            print(f\"Processed {class_idx+1}/{len(class_folders)} | Current accuracy: {100*correct/(class_idx+1):.1f}%\")\n",
    "    \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Accuracy: {100 * correct/len(class_folders):.2f}%\")\n",
    "print(f\"Time: {total_time:.2f} seconds\")\n",
    "print(f\"Speed: {len(class_folders)/total_time:.1f} images/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb0bb6-dfbc-410c-b9c6-e5e646517d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
